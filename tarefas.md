# üöÄ ROADMAP T√âCNICO - GT-Vision Split-Brain Architecture

**Meta:** MVP para 250 c√¢meras at√© final de Janeiro 2025  
**Arquitetura:** Split-Brain (segrega√ß√£o total de tr√°fego de v√≠deo vs API)

---

## üìã FASE 1: INFRAESTRUTURA CORE (Semana 1-2)

### 1.1 Implementar HAProxy como Load Balancer Principal
**Objetivo:** Segregar tr√°fego de v√≠deo do tr√°fego de API na entrada.

**Tarefas:**
- [ ] Criar `haproxy/haproxy.cfg` com ACLs para detectar rotas de v√≠deo
- [ ] Configurar backend para MediaMTX (porta 8888 HLS, 8889 WebRTC, 8554 RTSP)
- [ ] Configurar backend para API (Kong/WAF ‚Üí Gateway ‚Üí Django)
- [ ] Configurar backend para Frontend (Nginx est√°tico)
- [ ] Adicionar health checks para todos backends
- [ ] Configurar sticky sessions para WebRTC
- [ ] Adicionar ao `docker-compose.yml` como servi√ßo principal (porta 80/443)

**Arquivos a criar/modificar:**
```
haproxy/
  ‚îú‚îÄ‚îÄ haproxy.cfg          # Configura√ß√£o principal
  ‚îî‚îÄ‚îÄ Dockerfile           # Se necess√°rio customiza√ß√£o
docker-compose.yml         # Adicionar servi√ßo haproxy
.env                       # Vari√°veis HAProxy
```

**Regras de roteamento (ACLs):**
```haproxy
# CONFIGUR√ÅVEL: Ajustar paths conforme necess√°rio
acl is_video path_beg /hls/ /stream/ /ws/live/
acl is_video path_end .m3u8 .ts .mp4
acl is_rtsp dst_port 8554
acl is_api path_beg /api/ /admin/ /fast-api/
```

**Valida√ß√£o:**
- [ ] `curl http://localhost/hls/cam_1/index.m3u8` ‚Üí MediaMTX direto
- [ ] `curl http://localhost/api/cameras/` ‚Üí Gateway ‚Üí Django
- [ ] Verificar logs HAProxy: tr√°fego segregado corretamente

---

### 1.2 Otimizar MediaMTX para 250 C√¢meras
**Objetivo:** Garantir que MediaMTX suporte carga sem gargalos.

**Tarefas:**
- [ ] Ajustar `mediamtx.yml` para alta concorr√™ncia
- [ ] Configurar grava√ß√£o em disco com rota√ß√£o autom√°tica (7 dias)
- [ ] Habilitar API de m√©tricas (porta 9998)
- [ ] Configurar paths din√¢micos para c√¢meras (`cam_{id}`)
- [ ] Testar reconex√£o autom√°tica de streams RTSP
- [ ] Configurar HLS com segmentos otimizados

**Configura√ß√µes cr√≠ticas:**
```yaml
# CONFIGUR√ÅVEL: Ajustar conforme hardware
readTimeout: 10s
writeTimeout: 10s
writeQueueSize: 1024        # Aumentado de 512 para 250 c√¢meras

# HLS otimizado
hlsSegmentDuration: 2s      # CONFIGUR√ÅVEL: 1s=baixa lat√™ncia, 2s=menos carga
hlsSegmentCount: 3          # CONFIGUR√ÅVEL: Menor buffer, menos mem√≥ria
hlsSegmentMaxSize: 50M

# Grava√ß√£o
record: yes
recordPath: /recordings/%path/%Y-%m-%d_%H-%M-%S
recordFormat: fmp4
recordDeleteAfter: 7d       # CONFIGUR√ÅVEL: Reten√ß√£o de v√≠deo
```

**Valida√ß√£o:**
- [ ] Testar 10 c√¢meras simult√¢neas
- [ ] Verificar uso de CPU/RAM com `docker stats`
- [ ] Confirmar grava√ß√µes em `/recordings`
- [ ] Testar API: `curl http://mediamtx:9997/v3/paths/list`

---

### 1.3 Configurar Nginx como Servidor Est√°tico
**Objetivo:** Nginx serve apenas frontend e arquivos est√°ticos (n√£o faz proxy de v√≠deo).

**Tarefas:**
- [ ] Simplificar `nginx/nginx.conf` removendo proxies de v√≠deo
- [ ] Manter apenas: frontend, /static/, /media/
- [ ] Configurar cache agressivo para assets (7 dias)
- [ ] Adicionar compress√£o gzip/brotli
- [ ] Configurar HTTP/2

**Novo nginx.conf (simplificado):**
```nginx
# CONFIGUR√ÅVEL: worker_connections para mais clientes
worker_processes auto;
events {
    worker_connections 2048;  # Reduzido, n√£o serve mais v√≠deo
}

http {
    # Cache de assets
    location /static/ {
        alias /var/www/static/;
        expires 7d;             # CONFIGUR√ÅVEL: Cache de assets
        add_header Cache-Control "public, immutable";
    }
    
    # Frontend SPA
    location / {
        root /var/www/frontend;
        try_files $uri $uri/ /index.html;
    }
}
```

**Valida√ß√£o:**
- [ ] Frontend carrega em `http://localhost`
- [ ] Assets est√°ticos servidos com cache headers
- [ ] Verificar que v√≠deo N√ÉO passa por Nginx

---

## üìã FASE 2: BACKEND & INGEST√ÉO (Semana 2-3)

### 2.1 Otimizar Workers de IA (Extra√ß√£o de Frames)
**Objetivo:** Remover FFmpeg do Gateway, criar workers dedicados leves.

**Tarefas:**
- [ ] Criar `backend/apps/ai_workers/frame_extractor.py`
- [ ] Usar MediaMTX API para obter snapshot em vez de FFmpeg
- [ ] Configurar Celery queue dedicada: `ai_frame_extraction`
- [ ] Implementar rate limiting: 1 frame/segundo por c√¢mera
- [ ] Adicionar retry logic com backoff exponencial
- [ ] Enviar frame para servi√ßo IA externo via HTTP POST

**Novo worker (pseudo-c√≥digo):**
```python
# CONFIGUR√ÅVEL: FRAME_RATE = 1 frame/segundo
@celery_app.task(queue='ai_frame_extraction')
def extract_and_analyze_frame(camera_id: int):
    # Usa MediaMTX API em vez de FFmpeg
    snapshot_url = f"{MEDIAMTX_API}/v3/paths/get/cam_{camera_id}/snapshot"
    response = httpx.get(snapshot_url, timeout=5)
    
    if response.status_code == 200:
        frame_bytes = response.content
        # Envia para IA
        ai_response = httpx.post(AI_SERVICE_URL, files={'image': frame_bytes})
        # Processa resultado
        save_detection(camera_id, ai_response.json())
```

**Valida√ß√£o:**
- [ ] Worker consome <50MB RAM por c√¢mera
- [ ] CPU <10% por worker (sem FFmpeg)
- [ ] Lat√™ncia <2s (captura ‚Üí detec√ß√£o salva)

---

### 2.2 Otimizar Ingest√£o de Detec√ß√µes (Gateway FastAPI)
**Objetivo:** Suportar >1000 detec√ß√µes/segundo sem perda.

**Tarefas:**
- [ ] Implementar batch insert no `gateway/main.py`
- [ ] Adicionar fila Redis para buffer (se DB lento)
- [ ] Usar connection pooling no PostgreSQL (PgBouncer)
- [ ] Adicionar √≠ndices no banco (camera_id, timestamp)
- [ ] Implementar rate limiting por c√¢mera (evitar spam)

**Otimiza√ß√£o de ingest√£o:**
```python
# CONFIGUR√ÅVEL: BATCH_SIZE para ajustar throughput vs lat√™ncia
BATCH_SIZE = 100
BATCH_TIMEOUT = 1.0  # segundos

# Buffer em mem√≥ria (ou Redis)
detection_buffer = []

@app.post("/fast-api/ingest/lpr")
async def ingest_lpr_detection(detection: LPRDetection):
    detection_buffer.append(detection)
    
    if len(detection_buffer) >= BATCH_SIZE:
        await flush_buffer()
    
    return {"status": "queued"}

async def flush_buffer():
    if not detection_buffer:
        return
    
    # Batch insert (muito mais r√°pido)
    query = detections_table.insert()
    await database_writer.execute_many(query, detection_buffer)
    detection_buffer.clear()
```

**Valida√ß√£o:**
- [ ] Teste de carga: 1000 req/s com Locust
- [ ] Lat√™ncia p95 <50ms
- [ ] Zero perda de dados

---

### 2.3 Implementar PgBouncer (Connection Pooling)
**Objetivo:** Reduzir overhead de conex√µes ao PostgreSQL.

**Tarefas:**
- [ ] Adicionar servi√ßo `pgbouncer` ao `docker-compose.yml`
- [ ] Configurar pool de 100 conex√µes
- [ ] Apontar Django e Gateway para PgBouncer (porta 6432)
- [ ] Configurar modo `transaction` (melhor performance)

**docker-compose.yml:**
```yaml
pgbouncer:
  image: pgbouncer/pgbouncer:latest
  environment:
    - DATABASES_HOST=postgres_db
    - DATABASES_PORT=5432
    - DATABASES_USER=${POSTGRES_USER}
    - DATABASES_PASSWORD=${POSTGRES_PASSWORD}
    - DATABASES_DBNAME=${POSTGRES_DB}
    - PGBOUNCER_POOL_MODE=transaction
    - PGBOUNCER_MAX_CLIENT_CONN=1000    # CONFIGUR√ÅVEL
    - PGBOUNCER_DEFAULT_POOL_SIZE=25    # CONFIGUR√ÅVEL
```

**Valida√ß√£o:**
- [ ] Django conecta via PgBouncer
- [ ] Verificar `SHOW POOLS;` no PgBouncer
- [ ] Lat√™ncia de queries mantida ou melhorada

---

### 2.4 Otimizar Queries Django (Gargalos Conhecidos)
**Objetivo:** Reduzir lat√™ncia de listagens e dashboards.

**Tarefas:**
- [ ] Adicionar `select_related()` e `prefetch_related()` em ViewSets
- [ ] Criar √≠ndices compostos no PostgreSQL
- [ ] Usar `only()` e `defer()` para reduzir campos carregados
- [ ] Implementar pagina√ß√£o cursor-based para listas grandes
- [ ] Cachear queries pesadas no Redis (TTL 5s)

**√çndices cr√≠ticos:**
```sql
-- CONFIGUR√ÅVEL: Ajustar conforme queries mais frequentes
CREATE INDEX CONCURRENTLY idx_deteccoes_camera_ts 
  ON deteccoes(camera_id, timestamp DESC);

CREATE INDEX CONCURRENTLY idx_deteccoes_ts 
  ON deteccoes(timestamp DESC) 
  WHERE timestamp > NOW() - INTERVAL '7 days';

CREATE INDEX CONCURRENTLY idx_cameras_ativa 
  ON cameras(ativa) 
  WHERE ativa = true;
```

**Valida√ß√£o:**
- [ ] `EXPLAIN ANALYZE` em queries lentas
- [ ] Lat√™ncia de listagem <100ms
- [ ] Dashboard carrega em <500ms

---

## üìã FASE 3: FRONTEND (Semana 3)

### 3.1 Otimizar Bundle Size (Code Splitting)
**Objetivo:** Reduzir bundle de >2MB para <500KB (gzipped).

**Tarefas:**
- [ ] Analisar bundle com `npm run build -- --analyze`
- [ ] Implementar lazy loading de rotas
- [ ] Remover bibliotecas n√£o utilizadas
- [ ] Substituir bibliotecas pesadas por alternativas leves
- [ ] Configurar tree-shaking no Vite

**Otimiza√ß√µes:**
```typescript
// Lazy loading de p√°ginas
const Dashboard = lazy(() => import('./pages/Dashboard'));
const Cameras = lazy(() => import('./pages/Cameras'));

// Remover libs pesadas
// ‚ùå moment.js (500KB) ‚Üí ‚úÖ date-fns (10KB)
// ‚ùå lodash completo ‚Üí ‚úÖ lodash-es (tree-shakeable)
```

**Valida√ß√£o:**
- [ ] Bundle principal <200KB (gzipped)
- [ ] Chunks de rotas <100KB cada
- [ ] Lighthouse score >90

---

### 3.2 Otimizar Player de V√≠deo (HLS.js)
**Objetivo:** Player leve com overlay de detec√ß√µes via Canvas.

**Tarefas:**
- [ ] Usar HLS.js nativo (sem wrappers pesados)
- [ ] Implementar Canvas overlay para bounding boxes
- [ ] Adicionar fallback para WebRTC (baixa lat√™ncia)
- [ ] Implementar lazy loading de players (s√≥ carrega quando vis√≠vel)
- [ ] Otimizar re-renders com `React.memo()`

**Player otimizado:**
```typescript
// CONFIGUR√ÅVEL: HLS_BUFFER_SIZE para ajustar lat√™ncia
const HLS_CONFIG = {
  maxBufferLength: 10,        // CONFIGUR√ÅVEL: Menor = menos lat√™ncia
  maxMaxBufferLength: 20,
  liveSyncDuration: 3,
};

const VideoPlayer = React.memo(({ cameraId }) => {
  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  
  // Renderiza bounding boxes no Canvas (n√£o no DOM)
  const drawDetections = useCallback((detections) => {
    const ctx = canvasRef.current?.getContext('2d');
    // ... desenha ret√¢ngulos
  }, []);
  
  return (
    <>
      <video ref={videoRef} />
      <canvas ref={canvasRef} />
    </>
  );
});
```

**Valida√ß√£o:**
- [ ] Player carrega em <1s
- [ ] Overlay de detec√ß√µes sem lag
- [ ] Suporta 16 streams simult√¢neos sem travar

---

### 3.3 Implementar Virtual Scrolling (Listas Grandes)
**Objetivo:** Renderizar apenas itens vis√≠veis em listas de c√¢meras/detec√ß√µes.

**Tarefas:**
- [ ] Instalar `@tanstack/react-virtual`
- [ ] Implementar em lista de c√¢meras
- [ ] Implementar em lista de detec√ß√µes
- [ ] Adicionar skeleton loading

**Valida√ß√£o:**
- [ ] Lista de 1000 itens renderiza instantaneamente
- [ ] Scroll suave (60fps)

---

## üìã FASE 4: OBSERVABILIDADE & TESTES (Semana 4)

### 4.1 Implementar Prometheus + Grafana
**Objetivo:** M√©tricas centralizadas para identificar gargalos.

**Tarefas:**
- [ ] Adicionar Prometheus ao `docker-compose.yml`
- [ ] Configurar exporters: node_exporter, postgres_exporter, redis_exporter
- [ ] Expor m√©tricas do Django (django-prometheus)
- [ ] Expor m√©tricas do MediaMTX (porta 9998)
- [ ] Criar dashboards Grafana: CPU, RAM, Rede, Lat√™ncia, Throughput

**docker-compose.yml:**
```yaml
prometheus:
  image: prom/prometheus:latest
  volumes:
    - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    - prometheus_data:/prometheus
  ports:
    - "9090:9090"

grafana:
  image: grafana/grafana:latest
  volumes:
    - grafana_data:/var/lib/grafana
    - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
  ports:
    - "3000:3000"
```

**Valida√ß√£o:**
- [ ] M√©tricas vis√≠veis em Prometheus
- [ ] Dashboards funcionais em Grafana
- [ ] Alertas configurados (CPU >80%, Disco >85%)

---

### 4.2 Testes de Carga (Locust)
**Objetivo:** Validar que sistema suporta 250 c√¢meras + 100 usu√°rios.

**Tarefas:**
- [ ] Criar `tests/load/api_load.py` (Locust)
- [ ] Simular 100 usu√°rios acessando dashboard
- [ ] Simular 1000 detec√ß√µes/segundo
- [ ] Simular 50 streams simult√¢neos
- [ ] Medir lat√™ncia p95, p99
- [ ] Identificar gargalos

**Cen√°rios de teste:**
```python
# CONFIGUR√ÅVEL: Ajustar conforme meta de performance
class APIUser(HttpUser):
    wait_time = between(1, 3)
    
    @task(3)
    def view_dashboard(self):
        self.client.get("/api/analytics/dashboard/")
    
    @task(2)
    def list_cameras(self):
        self.client.get("/api/cameras/")
    
    @task(1)
    def view_detections(self):
        self.client.get("/api/deteccoes/")
```

**Valida√ß√£o:**
- [ ] API: p95 <100ms, p99 <200ms
- [ ] Ingest√£o: >1000 req/s sem erros
- [ ] V√≠deo: lat√™ncia <3s (HLS)
- [ ] Zero crashes ou timeouts

---

### 4.3 Testes de Resili√™ncia
**Objetivo:** Sistema se recupera de falhas automaticamente.

**Tarefas:**
- [ ] Testar queda de PostgreSQL (failover para r√©plica)
- [ ] Testar queda de Redis (reconex√£o autom√°tica)
- [ ] Testar queda de MediaMTX (reconex√£o de c√¢meras)
- [ ] Testar queda de c√¢mera (health check detecta)
- [ ] Testar sobrecarga (rate limiting funciona)

**Valida√ß√£o:**
- [ ] Downtime <30s em falhas de componentes
- [ ] Dados n√£o s√£o perdidos
- [ ] Alertas s√£o disparados

---

## üìä CHECKLIST FINAL (MVP Ready)

### Performance
- [ ] API: p95 <100ms
- [ ] V√≠deo HLS: lat√™ncia <3s
- [ ] V√≠deo WebRTC: lat√™ncia <500ms
- [ ] Ingest√£o: >1000 detec√ß√µes/s
- [ ] Frontend: Lighthouse >90

### Escala
- [ ] 250 c√¢meras simult√¢neas est√°veis
- [ ] 100 usu√°rios concorrentes
- [ ] 50 streams simult√¢neos por usu√°rio

### Recursos
- [ ] CPU <70% (carga normal)
- [ ] RAM <80% (carga normal)
- [ ] Disco <85%
- [ ] Rede <80% capacidade

### Observabilidade
- [ ] Prometheus coletando m√©tricas
- [ ] Grafana com dashboards
- [ ] Alertas configurados
- [ ] Logs centralizados

### Seguran√ßa
- [ ] HTTPS em produ√ß√£o
- [ ] JWT funcionando
- [ ] Rate limiting ativo
- [ ] Senhas criptografadas

---

## üîß CONFIGURA√á√ïES PARA AJUSTE FINO

### HAProxy
```
# CONFIGUR√ÅVEL: Timeouts
timeout connect 5s
timeout client 30s
timeout server 30s
timeout tunnel 1h    # Para WebRTC/WebSocket
```

### MediaMTX
```yaml
# CONFIGUR√ÅVEL: Performance vs Lat√™ncia
hlsSegmentDuration: 2s    # 1s=baixa lat√™ncia, 4s=menos CPU
writeQueueSize: 1024      # Aumentar se drops de frames
```

### PostgreSQL
```sql
-- CONFIGUR√ÅVEL: Tuning para 250 c√¢meras
shared_buffers = 2GB
effective_cache_size = 6GB
work_mem = 16MB
maintenance_work_mem = 512MB
max_connections = 200
```

### Redis
```
# CONFIGUR√ÅVEL: Mem√≥ria
maxmemory 512mb           # Aumentar se cache misses
maxmemory-policy allkeys-lru
```

### Celery
```python
# CONFIGUR√ÅVEL: Workers
CELERY_WORKER_CONCURRENCY = 4    # CPU cores
CELERY_WORKER_PREFETCH_MULTIPLIER = 2
```

---

## üìÖ CRONOGRAMA SUGERIDO

| Semana | Fase | Entregas |
|--------|------|----------|
| 1 | Infra Core | HAProxy, MediaMTX otimizado, Nginx simplificado |
| 2 | Backend | Workers IA, PgBouncer, Queries otimizadas |
| 3 | Frontend | Bundle otimizado, Player leve, Virtual scroll |
| 4 | Observabilidade | Prometheus, Grafana, Testes de carga |

**Data de entrega:** Final de Janeiro 2025

---

## üö® RISCOS E MITIGA√á√ïES

| Risco | Impacto | Mitiga√ß√£o |
|-------|---------|-----------|
| MediaMTX n√£o aguenta 250 c√¢meras | Alto | Testar com 50, 100, 150 incrementalmente |
| Disco enche r√°pido (8TB/semana) | Alto | Implementar limpeza autom√°tica, alertas |
| Lat√™ncia de rede alta | M√©dio | CDN para v√≠deo, compress√£o |
| PostgreSQL lento | Alto | PgBouncer, √≠ndices, r√©plicas de leitura |
| Frontend pesado | M√©dio | Code splitting, lazy loading |

---

**PR√ìXIMO PASSO:** Come√ßar pela Fase 1.1 (HAProxy)
