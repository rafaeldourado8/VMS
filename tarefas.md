# üöÄ ROADMAP T√âCNICO - GT-Vision Split-Brain Architecture

**Meta:** MVP para 250 c√¢meras at√© final de Janeiro 2025  
**Arquitetura:** Split-Brain com GPU Workers dedicados para IA

---

## üìê VIS√ÉO GERAL DA ARQUITETURA

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                              EDGE / CDN                                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                       ‚îÇ
‚îÇ  ‚îÇ  CloudFlare  ‚îÇ    ‚îÇ   WAF/DDoS   ‚îÇ                                       ‚îÇ
‚îÇ  ‚îÇ    (CDN)     ‚îÇ    ‚îÇ  Protection  ‚îÇ                                       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                               ‚îÇ
‚îÇ                    ‚ñº                                                         ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                                ‚îÇ
‚îÇ         ‚îÇ  HAProxy Nodes   ‚îÇ ‚Üê Stats Dashboard :8404                        ‚îÇ
‚îÇ         ‚îÇ  (Load Balancer) ‚îÇ                                                ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                     ‚îÇ
        ‚ñº                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  /video_api   ‚îÇ     ‚îÇ static_files  ‚îÇ
‚îÇ     Kong      ‚îÇ     ‚îÇ    Nginx      ‚îÇ
‚îÇ (API Gateway) ‚îÇ     ‚îÇ   (:8080)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        API GATEWAY (Kong/KongDB)                            ‚îÇ
‚îÇ  Rate Limiting ‚îÇ JWT Auth ‚îÇ Routing ‚îÇ SSL Termination                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                            ‚îÇ
‚îÇ  ‚îÇ Kong DB‚îÇ ‚îÇCassandra‚îÇ ‚îÇAuth/JWT ‚îÇ ‚îÇ Logging  ‚îÇ                            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       CAMADA SET 01 (API Workers)                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ Backend Django  ‚îÇ  ‚îÇ Auth/Identity   ‚îÇ  ‚îÇ Gateway FastAPI ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ REST API     ‚îÇ  ‚îÇ   Keycloak      ‚îÇ  ‚îÇ  ‚Ä¢ Bulk Ingest  ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Admin        ‚îÇ  ‚îÇ  ‚Ä¢ SSO          ‚îÇ  ‚îÇ  ‚Ä¢ WebSocket    ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ ORM          ‚îÇ  ‚îÇ  ‚Ä¢ LDAP         ‚îÇ  ‚îÇ  ‚Ä¢ Async        ‚îÇ              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       CAMADA SET 02 (AI Workers)                            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ              Frame Grabber Service (FastAPI + AI)                    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ         Alta Disponibilidade - Suporte GPU Local + AWS               ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                                                                       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ     GPU WORKERS (Local)     ‚îÇ  ‚îÇ    AWS WORKERS (EC2)        ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇGPU #1 ‚îÇ ‚îÇGPU #2 ‚îÇ       ‚îÇ  ‚îÇ  ‚îÇEC2 #1 ‚îÇ ‚îÇEC2 #N ‚îÇ        ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇYOLO   ‚îÇ ‚îÇYOLO   ‚îÇ       ‚îÇ  ‚îÇ  ‚îÇRekog. ‚îÇ ‚îÇRekog. ‚îÇ        ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇTF/LPR ‚îÇ ‚îÇTF/LPR ‚îÇ       ‚îÇ  ‚îÇ  ‚îÇAPI    ‚îÇ ‚îÇAPI    ‚îÇ        ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îÇCUDA   ‚îÇ ‚îÇCUDA   ‚îÇ       ‚îÇ  ‚îÇ  ‚îÇ       ‚îÇ ‚îÇ       ‚îÇ        ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ     ~50ms latency          ‚îÇ  ‚îÇ     ~200ms latency          ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ     Custo fixo             ‚îÇ  ‚îÇ     Pay-per-use             ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                         ‚Üì                                            ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ     HYBRID PROVIDER               ‚îÇ                       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ  ‚Ä¢ Primary: GPU (baixa lat√™ncia)  ‚îÇ                       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ  ‚Ä¢ Fallback: AWS (alta escala)    ‚îÇ                       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ         ‚îÇ  ‚Ä¢ Circuit Breaker autom√°tico     ‚îÇ                       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                         ‚Üì                                            ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ              Batch Processing (Frames + Detections)                  ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       CAMADA DE MENSAGERIA                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ
‚îÇ  ‚îÇ PostgreSQL      ‚îÇ  ‚îÇ Redis Cluster   ‚îÇ  ‚îÇ MinIO (S3)      ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Primary      ‚îÇ  ‚îÇ  ‚Ä¢ Cache API    ‚îÇ  ‚îÇ Object Storage  ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Replica RO   ‚îÇ  ‚îÇ  ‚Ä¢ Pub/Sub      ‚îÇ  ‚îÇ  ‚Ä¢ Frames       ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ PgBouncer    ‚îÇ  ‚îÇ  ‚Ä¢ Sessions     ‚îÇ  ‚îÇ  ‚Ä¢ Recordings   ‚îÇ              ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Backup       ‚îÇ  ‚îÇ  ‚Ä¢ Rate Limit   ‚îÇ  ‚îÇ  ‚Ä¢ Replica√ß√£o   ‚îÇ              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                       OBSERVABILIDADE                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                 ‚îÇ
‚îÇ  ‚îÇPrometheus ‚îÇ  ‚îÇ Grafana   ‚îÇ  ‚îÇ   Loki    ‚îÇ  ‚îÇ  Jaeger   ‚îÇ                 ‚îÇ
‚îÇ  ‚îÇ (Metrics) ‚îÇ  ‚îÇ(Dashboard)‚îÇ  ‚îÇ  (Logs)   ‚îÇ  ‚îÇ (Tracing) ‚îÇ                 ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îÇ
‚îÇ        ‚îÇ                                                                     ‚îÇ
‚îÇ        ‚ñº                                                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ              Alertmanager ‚Üí PagerDuty / Slack                        ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ                    AWS CloudWatch (EC2 Workers)                      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ              ‚Ä¢ Billing Alerts ‚Ä¢ Auto Scaling Metrics                 ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìã FASE 1: INFRAESTRUTURA CORE (Semana 1-2)

### ~~1.1 Implementar HAProxy como Load Balancer Principal~~ ‚úÖ
**Objetivo:** Segregar tr√°fego de v√≠deo do tr√°fego de API na entrada.

**Tarefas:**
- [x] Criar `haproxy/haproxy.cfg` com ACLs para detectar rotas de v√≠deo
- [x] Configurar backend para MediaMTX (porta 8888 HLS, 8889 WebRTC, 8554 RTSP)
- [x] Configurar backend para API (Kong/WAF ‚Üí Gateway ‚Üí Django)
- [x] Configurar backend para Frontend (Nginx est√°tico)
- [x] Adicionar health checks para todos backends
- [x] Configurar sticky sessions para WebRTC
- [x] Adicionar ao `docker-compose.yml` como servi√ßo principal (porta 80/443)

**Implementado:** `haproxy/haproxy.cfg` + `docker-compose.yml`
- Split-brain: V√≠deo ‚Üí MediaMTX direto (bypass API)
- API ‚Üí Gateway ‚Üí Django
- Est√°ticos ‚Üí Nginx (porta 8080)
- Sticky sessions para WebRTC
- Health checks: 10s interval
- Stats dashboard: http://localhost:8404/stats

**Valida√ß√£o:**
- [x] HAProxy rodando na porta 80
- [x] ACLs segregando tr√°fego corretamente
- [x] Health checks funcionando

---

### ~~1.2 Otimizar MediaMTX para 250 C√¢meras~~ ‚úÖ
**Objetivo:** Garantir que MediaMTX suporte carga sem gargalos.

**Tarefas:**
- [x] Ajustar `mediamtx.yml` para alta concorr√™ncia
- [x] Configurar grava√ß√£o em disco com rota√ß√£o autom√°tica (7 dias)
- [x] Habilitar API de m√©tricas (porta 9998)
- [x] Configurar paths din√¢micos para c√¢meras (`cam_{id}`)
- [x] Testar reconex√£o autom√°tica de streams RTSP
- [x] Configurar HLS com segmentos otimizados

**Implementado:** `mediamtx.yml` otimizado
- writeQueueSize: 1024 (buffer para 250 c√¢meras)
- HLS: 2s segments, 3 count (equil√≠brio lat√™ncia/carga)
- Grava√ß√£o: fmp4, 1h segments, 7d reten√ß√£o
- API: porta 9997, Metrics: porta 9998
- maxReaders: 100 por stream
- sourceOnDemand: yes (economiza recursos)

**Valida√ß√£o:**
- [x] 6 c√¢meras reais configuradas e testadas
- [x] API funcionando (porta 9997)
- [x] Metrics habilitadas (porta 9998)
- [x] Grava√ß√µes em `/recordings` com rota√ß√£o 7d

---

### ~~1.3 Configurar Nginx como Servidor Est√°tico~~ ‚úÖ
**Objetivo:** Nginx serve apenas frontend e arquivos est√°ticos (n√£o faz proxy de v√≠deo).

**Tarefas:**
- [x] Simplificar `nginx/nginx.conf` removendo proxies de v√≠deo
- [x] Manter apenas: frontend, /static/, /media/
- [x] Configurar cache agressivo para assets (7 dias)
- [x] Adicionar compress√£o gzip/brotli
- [x] Configurar HTTP/2

**Implementado:** `nginx/nginx.simple.conf` (30 linhas vs 300)
- Apenas serve `/static/` e `/media/` na porta 8080
- Removidos todos os proxies (HAProxy faz roteamento direto)
- Economia: 90% mem√≥ria (~5MB vs ~50MB)

**Valida√ß√£o:**
- [x] Config validada com `nginx -t`
- [x] Assets est√°ticos servidos com cache headers
- [x] V√≠deo N√ÉO passa por Nginx (HAProxy ‚Üí MediaMTX direto)

---

### ~~1.4 Implementar Kong API Gateway~~ ‚úÖ
**Objetivo:** Substituir roteamento direto por API Gateway enterprise-grade.

**Tarefas:**
- [x] Adicionar Kong (DB-less mode) ao `docker-compose.yml`
- [x] Configurar rate limiting global e por rota
- [x] Configurar CORS
- [x] Configurar Prometheus metrics
- [x] Criar rotas para Django, Gateway FastAPI
- [x] Configurar health checks
- [x] Integrar com HAProxy
- [ ] Configurar JWT validation plugin (ap√≥s Keycloak)

**docker-compose.yml:**
```yaml
kong-database:
  image: postgres:15-alpine
  environment:
    POSTGRES_USER: kong
    POSTGRES_DB: kong
    POSTGRES_PASSWORD: ${KONG_DB_PASSWORD}
  volumes:
    - kong_data:/var/lib/postgresql/data
  healthcheck:
    test: ["CMD", "pg_isready", "-U", "kong"]
    interval: 10s
    timeout: 5s
    retries: 5

kong-migrations:
  image: kong:3.5
  command: kong migrations bootstrap
  environment:
    KONG_DATABASE: postgres
    KONG_PG_HOST: kong-database
    KONG_PG_USER: kong
    KONG_PG_PASSWORD: ${KONG_DB_PASSWORD}
  depends_on:
    kong-database:
      condition: service_healthy

kong:
  image: kong:3.5
  environment:
    KONG_DATABASE: postgres
    KONG_PG_HOST: kong-database
    KONG_PG_USER: kong
    KONG_PG_PASSWORD: ${KONG_DB_PASSWORD}
    KONG_PROXY_ACCESS_LOG: /dev/stdout
    KONG_ADMIN_ACCESS_LOG: /dev/stdout
    KONG_PROXY_ERROR_LOG: /dev/stderr
    KONG_ADMIN_ERROR_LOG: /dev/stderr
    KONG_ADMIN_LISTEN: 0.0.0.0:8001
    KONG_ADMIN_GUI_LISTEN: 0.0.0.0:8002
  ports:
    - "8000:8000"   # Proxy
    - "8001:8001"   # Admin API
    - "8002:8002"   # Kong Manager GUI
  depends_on:
    kong-migrations:
      condition: service_completed_successfully
  healthcheck:
    test: ["CMD", "kong", "health"]
    interval: 10s
    timeout: 5s
    retries: 5
```

**Configura√ß√£o de rotas (via Admin API):**
```bash
# Criar servi√ßo Django
curl -X POST http://localhost:8001/services \
  --data name=django-api \
  --data url=http://django:8000

# Criar rota
curl -X POST http://localhost:8001/services/django-api/routes \
  --data paths[]=/api \
  --data strip_path=false

# Habilitar rate limiting
curl -X POST http://localhost:8001/services/django-api/plugins \
  --data name=rate-limiting \
  --data config.minute=100 \
  --data config.policy=local

# Habilitar JWT
curl -X POST http://localhost:8001/services/django-api/plugins \
  --data name=jwt
```

**Implementado:** `kong/kong.yml` + `docker-compose.yml` + `haproxy/haproxy.cfg`
- DB-less mode (sem PostgreSQL/Cassandra extra)
- Rate limiting: /api (100/min), /fast-api (1000/min), /admin (30/min)
- CORS configurado para frontend
- Prometheus metrics em /metrics
- Request/Correlation IDs para tracing
- HAProxy ‚Üí Kong ‚Üí Django/Gateway

**Valida√ß√£o:**
- [x] Kong rodando e acess√≠vel (:8000)
- [x] Kong Manager GUI funcionando (:8002)
- [x] Admin API funcionando (:8001)
- [x] Rate limiting configurado
- [x] CORS funcionando
- [x] HAProxy roteando para Kong
- [x] Health checks passando
- [x] Rota de static files funcionando (16/12/2024)
- [x] Django Admin acess√≠vel via Kong (16/12/2024)
- [ ] JWT validation (aguarda Keycloak)

---

### 1.5 Implementar Keycloak (Auth/Identity)
**Objetivo:** Centralizar autentica√ß√£o com SSO, LDAP, e OAuth2.

**Tarefas:**
- [ ] Adicionar Keycloak ao `docker-compose.yml`
- [ ] Configurar realm para GT-Vision
- [ ] Configurar client para frontend (public)
- [ ] Configurar client para backend (confidential)
- [ ] Integrar com Kong JWT plugin
- [ ] Configurar roles: admin, operator, viewer
- [ ] (Opcional) Integrar com LDAP/AD

**docker-compose.yml:**
```yaml
keycloak:
  image: quay.io/keycloak/keycloak:23.0
  command: start-dev
  environment:
    KC_DB: postgres
    KC_DB_URL: jdbc:postgresql://postgres_db:5432/keycloak
    KC_DB_USERNAME: ${POSTGRES_USER}
    KC_DB_PASSWORD: ${POSTGRES_PASSWORD}
    KEYCLOAK_ADMIN: admin
    KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD}
    KC_PROXY: edge
    KC_HOSTNAME_STRICT: false
  ports:
    - "8080:8080"
  depends_on:
    - postgres_db
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8080/health/ready"]
    interval: 10s
    timeout: 5s
    retries: 5
```

**Valida√ß√£o:**
- [ ] Keycloak Admin Console acess√≠vel
- [ ] Realm GT-Vision criado
- [ ] Clients configurados
- [ ] Login flow funcionando
- [ ] Tokens JWT sendo validados pelo Kong

---

## üìã FASE 2: BACKEND & SERVI√áO DE IA (Semana 2-3)

### 2.1 Criar Servi√ßo de IA com FastAPI (GPU Workers + AWS Rekognition)
**Objetivo:** Servi√ßo dedicado de alta disponibilidade para detec√ß√£o com suporte h√≠brido (GPU local ou AWS).

**Modos de Opera√ß√£o:**
| Modo | Onde Roda | Modelos | Custo | Lat√™ncia |
|------|-----------|---------|-------|----------|
| **GPU Local** | On-premise / EC2 GPU | YOLO + TensorFlow | Fixo (hardware) | ~50ms |
| **AWS Rekognition** | EC2 t3/c5 + API AWS | Rekognition API | Pay-per-use | ~200ms |
| **H√≠brido** | Ambos | Fallback autom√°tico | Otimizado | Vari√°vel |

**Estrutura:**
```
ai_service/
‚îú‚îÄ‚îÄ Dockerfile.gpu           # Para GPU workers locais
‚îú‚îÄ‚îÄ Dockerfile.cpu           # Para EC2 com Rekognition
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ requirements-aws.txt     # Boto3, etc
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI app
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Settings (AI_BACKEND: gpu|aws|hybrid)
‚îÇ   ‚îú‚îÄ‚îÄ providers/           # Abstra√ß√£o de providers de IA
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py          # Interface base
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gpu_provider.py  # YOLO + TensorFlow local
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aws_provider.py  # AWS Rekognition
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hybrid_provider.py # Fallback autom√°tico
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ yolo_detector.py   # YOLOv8/v11
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tf_classifier.py   # TensorFlow models
‚îÇ   ‚îú‚îÄ‚îÄ routers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ detection.py       # /detect endpoint
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ health.py          # /health endpoint
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ batch.py           # /batch endpoint
‚îÇ   ‚îú‚îÄ‚îÄ workers/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ frame_grabber.py   # Captura frames do MediaMTX
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ processor.py       # Pipeline de processamento
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ gpu_utils.py       # CUDA management
‚îÇ       ‚îú‚îÄ‚îÄ aws_utils.py       # AWS helpers
‚îÇ       ‚îî‚îÄ‚îÄ metrics.py         # Prometheus metrics
‚îú‚îÄ‚îÄ models/                    # Model weights (apenas GPU mode)
‚îÇ   ‚îú‚îÄ‚îÄ yolov8n.pt
‚îÇ   ‚îú‚îÄ‚îÄ yolov8s.pt
‚îÇ   ‚îî‚îÄ‚îÄ custom_lpr.pt
‚îú‚îÄ‚îÄ terraform/                 # IaC para EC2 workers
‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îî‚îÄ‚îÄ ec2-workers.tf
‚îî‚îÄ‚îÄ tests/
    ‚îî‚îÄ‚îÄ test_detection.py
```

**Tarefas:**

**Core:**
- [ ] Criar `ai_service/Dockerfile.gpu` com CUDA 12.x (workers GPU)
- [ ] Criar `ai_service/Dockerfile.cpu` para EC2 com Rekognition
- [ ] Implementar `main.py` com FastAPI + Uvicorn
- [ ] Implementar interface base `providers/base.py`
- [ ] Implementar endpoint `/detect` (single frame)
- [ ] Implementar endpoint `/batch` (m√∫ltiplos frames)
- [ ] Implementar endpoint `/health` com status do provider
- [ ] Adicionar m√©tricas Prometheus
- [ ] Configurar auto-scaling com r√©plicas

**GPU Provider (On-Premise / EC2 GPU):**
- [ ] Implementar `providers/gpu_provider.py`
- [ ] Implementar detector YOLO com batch processing
- [ ] Implementar detector TensorFlow para LPR
- [ ] Gerenciamento de mem√≥ria GPU

**AWS Provider (EC2 + Rekognition):**
- [ ] Implementar `providers/aws_provider.py`
- [ ] Integrar AWS Rekognition DetectLabels
- [ ] Integrar AWS Rekognition DetectText (para placas)
- [ ] Integrar AWS Rekognition DetectFaces (opcional)
- [ ] Implementar retry logic com exponential backoff
- [ ] Configurar AWS credentials via IAM Role (EC2)

**H√≠brido:**
- [ ] Implementar `providers/hybrid_provider.py`
- [ ] L√≥gica de fallback (GPU ‚Üí AWS se GPU falhar)
- [ ] Load balancing entre providers
- [ ] Circuit breaker para AWS (evitar custos em falhas)

**Infraestrutura AWS:**
- [ ] Criar Terraform para EC2 workers
- [ ] Configurar Auto Scaling Group
- [ ] Configurar IAM Role com permiss√µes Rekognition
- [ ] Configurar VPC endpoints para Rekognition (reduz lat√™ncia)

**Dockerfile.gpu (GPU Workers Locais/EC2 GPU):**
```dockerfile
FROM nvidia/cuda:12.2-cudnn8-runtime-ubuntu22.04

WORKDIR /app

# Instalar Python e depend√™ncias
RUN apt-get update && apt-get install -y \
    python3.11 python3-pip libgl1-mesa-glx libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

# Instalar depend√™ncias Python
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copiar c√≥digo
COPY app/ ./app/
COPY models/ ./models/

# Vari√°veis de ambiente
ENV PYTHONUNBUFFERED=1
ENV CUDA_VISIBLE_DEVICES=0
ENV AI_BACKEND=gpu

# Health check
HEALTHCHECK --interval=30s --timeout=10s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Rodar com m√∫ltiplos workers
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "2"]
```

**Dockerfile.cpu (EC2 com AWS Rekognition):**
```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Instalar depend√™ncias m√≠nimas
RUN apt-get update && apt-get install -y \
    libgl1-mesa-glx libglib2.0-0 curl \
    && rm -rf /var/lib/apt/lists/*

# Instalar depend√™ncias Python (sem CUDA)
COPY requirements-aws.txt .
RUN pip install --no-cache-dir -r requirements-aws.txt

# Copiar c√≥digo (sem modelos pesados)
COPY app/ ./app/

# Vari√°veis de ambiente
ENV PYTHONUNBUFFERED=1
ENV AI_BACKEND=aws
ENV AWS_DEFAULT_REGION=us-east-1

# Health check
HEALTHCHECK --interval=30s --timeout=10s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Rodar com mais workers (CPU √© barato)
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

**requirements.txt (GPU mode):**
```
fastapi==0.109.0
uvicorn[standard]==0.27.0
ultralytics==8.1.0
torch==2.2.0
torchvision==0.17.0
tensorflow==2.15.0
opencv-python-headless==4.9.0.80
numpy==1.26.3
httpx==0.26.0
prometheus-client==0.19.0
python-multipart==0.0.6
Pillow==10.2.0
redis==5.0.1
```

**requirements-aws.txt (AWS Rekognition mode):**
```
fastapi==0.109.0
uvicorn[standard]==0.27.0
boto3==1.34.0
botocore==1.34.0
opencv-python-headless==4.9.0.80
numpy==1.26.3
httpx==0.26.0
prometheus-client==0.19.0
python-multipart==0.0.6
Pillow==10.2.0
redis==5.0.1
aioboto3==12.0.0
```

**app/main.py:**
```python
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
import asyncio
from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST
from starlette.responses import Response

from app.config import settings
from app.providers.base import AIProvider
from app.providers.gpu_provider import GPUProvider
from app.providers.aws_provider import AWSProvider
from app.providers.hybrid_provider import HybridProvider

# M√©tricas Prometheus
REQUESTS_TOTAL = Counter('ai_requests_total', 'Total de requisi√ß√µes', ['endpoint', 'status', 'provider'])
INFERENCE_TIME = Histogram('ai_inference_seconds', 'Tempo de infer√™ncia', ['provider'])

# Provider de IA (carregado na inicializa√ß√£o)
ai_provider: AIProvider = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Carrega provider de IA baseado na configura√ß√£o."""
    global ai_provider
    
    print(f"üöÄ Inicializando AI Provider: {settings.AI_BACKEND}")
    
    if settings.AI_BACKEND == "gpu":
        ai_provider = GPUProvider(
            yolo_model=settings.YOLO_MODEL_PATH,
            lpr_model=settings.LPR_MODEL_PATH,
            device=settings.GPU_DEVICE,
            confidence=settings.CONFIDENCE_THRESHOLD
        )
    elif settings.AI_BACKEND == "aws":
        ai_provider = AWSProvider(
            region=settings.AWS_REGION,
            min_confidence=settings.CONFIDENCE_THRESHOLD * 100  # AWS usa 0-100
        )
    elif settings.AI_BACKEND == "hybrid":
        ai_provider = HybridProvider(
            primary=GPUProvider(...),
            fallback=AWSProvider(...),
            fallback_on_error=True
        )
    else:
        raise ValueError(f"AI_BACKEND inv√°lido: {settings.AI_BACKEND}")
    
    await ai_provider.initialize()
    print(f"‚úÖ AI Provider '{settings.AI_BACKEND}' inicializado!")
    
    yield
    
    # Cleanup
    print("üõë Desligando AI Provider...")
    await ai_provider.shutdown()

app = FastAPI(
    title="GT-Vision AI Service",
    description="Servi√ßo de detec√ß√£o com suporte a GPU local e AWS Rekognition",
    version="2.0.0",
    lifespan=lifespan
)

@app.get("/metrics")
async def metrics():
    """Endpoint para Prometheus."""
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)

@app.get("/health")
async def health():
    """Health check com status do provider."""
    status = await ai_provider.health_check()
    return {
        "status": "healthy" if status["ok"] else "unhealthy",
        "provider": settings.AI_BACKEND,
        "details": status
    }

@app.post("/detect/frame")
async def detect_frame(
    file: UploadFile = File(...),
    camera_id: int = None,
    detect_plates: bool = True
):
    """
    Detecta objetos em um frame.
    Usa GPU local ou AWS Rekognition baseado na configura√ß√£o.
    """
    try:
        contents = await file.read()
        
        with INFERENCE_TIME.labels(provider=settings.AI_BACKEND).time():
            detections = await ai_provider.detect(
                image_bytes=contents,
                detect_text=detect_plates
            )
        
        REQUESTS_TOTAL.labels(
            endpoint='detect', 
            status='success',
            provider=settings.AI_BACKEND
        ).inc()
        
        return {
            "camera_id": camera_id,
            "provider": settings.AI_BACKEND,
            "detections": detections,
            "count": len(detections)
        }
        
    except Exception as e:
        REQUESTS_TOTAL.labels(
            endpoint='detect', 
            status='error',
            provider=settings.AI_BACKEND
        ).inc()
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/batch/detect")
async def batch_detect(
    files: list[UploadFile] = File(...),
    camera_ids: list[int] = None
):
    """Processa m√∫ltiplos frames em batch."""
    images = [await f.read() for f in files]
    
    with INFERENCE_TIME.labels(provider=settings.AI_BACKEND).time():
        results = await ai_provider.detect_batch(images)
    
    return {
        "provider": settings.AI_BACKEND,
        "results": [
            {"camera_id": cid, "detections": dets}
            for cid, dets in zip(camera_ids or range(len(results)), results)
        ]
    }
```

**app/providers/base.py:**
```python
from abc import ABC, abstractmethod
from typing import List, Dict, Any

class AIProvider(ABC):
    """Interface base para providers de IA."""
    
    @abstractmethod
    async def initialize(self) -> None:
        """Inicializa o provider (carrega modelos, conecta APIs, etc)."""
        pass
    
    @abstractmethod
    async def shutdown(self) -> None:
        """Cleanup do provider."""
        pass
    
    @abstractmethod
    async def detect(
        self, 
        image_bytes: bytes, 
        detect_text: bool = False
    ) -> List[Dict[str, Any]]:
        """
        Detecta objetos em uma imagem.
        
        Returns:
            Lista de detec√ß√µes: [{"class": str, "confidence": float, "bbox": [x1,y1,x2,y2], "text": str|None}]
        """
        pass
    
    @abstractmethod
    async def detect_batch(
        self, 
        images: List[bytes]
    ) -> List[List[Dict[str, Any]]]:
        """Detecta objetos em m√∫ltiplas imagens."""
        pass
    
    @abstractmethod
    async def health_check(self) -> Dict[str, Any]:
        """Retorna status de sa√∫de do provider."""
        pass
```

**app/providers/aws_provider.py:**
```python
import aioboto3
from typing import List, Dict, Any
from app.providers.base import AIProvider
import io

class AWSProvider(AIProvider):
    """Provider usando AWS Rekognition."""
    
    def __init__(self, region: str = "us-east-1", min_confidence: float = 50.0):
        self.region = region
        self.min_confidence = min_confidence
        self.session = None
        self.client = None
    
    async def initialize(self) -> None:
        self.session = aioboto3.Session()
        # Cliente ser√° criado por requisi√ß√£o (connection pooling do aioboto3)
        print(f"AWS Provider inicializado (regi√£o: {self.region})")
    
    async def shutdown(self) -> None:
        pass  # aioboto3 gerencia conex√µes automaticamente
    
    async def detect(
        self, 
        image_bytes: bytes, 
        detect_text: bool = False
    ) -> List[Dict[str, Any]]:
        detections = []
        
        async with self.session.client('rekognition', region_name=self.region) as client:
            # Detectar objetos/labels
            labels_response = await client.detect_labels(
                Image={'Bytes': image_bytes},
                MinConfidence=self.min_confidence,
                Features=['GENERAL_LABELS']
            )
            
            for label in labels_response.get('Labels', []):
                for instance in label.get('Instances', []):
                    bbox = instance.get('BoundingBox', {})
                    detections.append({
                        "class": label['Name'].lower(),
                        "confidence": label['Confidence'] / 100,
                        "bbox": self._convert_bbox(bbox),
                        "source": "rekognition"
                    })
            
            # Detectar texto (placas)
            if detect_text:
                text_response = await client.detect_text(
                    Image={'Bytes': image_bytes}
                )
                
                for text in text_response.get('TextDetections', []):
                    if text['Type'] == 'LINE' and text['Confidence'] > self.min_confidence:
                        bbox = text.get('Geometry', {}).get('BoundingBox', {})
                        detections.append({
                            "class": "plate",
                            "confidence": text['Confidence'] / 100,
                            "bbox": self._convert_bbox(bbox),
                            "text": text['DetectedText'],
                            "source": "rekognition"
                        })
        
        return detections
    
    async def detect_batch(
        self, 
        images: List[bytes]
    ) -> List[List[Dict[str, Any]]]:
        """Processa m√∫ltiplas imagens em paralelo."""
        import asyncio
        tasks = [self.detect(img, detect_text=True) for img in images]
        return await asyncio.gather(*tasks)
    
    async def health_check(self) -> Dict[str, Any]:
        try:
            async with self.session.client('rekognition', region_name=self.region) as client:
                # Chamada leve para verificar conectividade
                await client.describe_projects(MaxResults=1)
            return {"ok": True, "provider": "aws", "region": self.region}
        except Exception as e:
            return {"ok": False, "provider": "aws", "error": str(e)}
    
    def _convert_bbox(self, aws_bbox: dict) -> List[float]:
        """Converte bbox AWS (normalizado) para [x1, y1, x2, y2]."""
        if not aws_bbox:
            return [0, 0, 0, 0]
        # AWS retorna: Left, Top, Width, Height (0-1)
        # Convertemos para: x1, y1, x2, y2 (0-1)
        return [
            aws_bbox.get('Left', 0),
            aws_bbox.get('Top', 0),
            aws_bbox.get('Left', 0) + aws_bbox.get('Width', 0),
            aws_bbox.get('Top', 0) + aws_bbox.get('Height', 0)
        ]
```

**app/providers/hybrid_provider.py:**
```python
from typing import List, Dict, Any
from app.providers.base import AIProvider
import asyncio

class HybridProvider(AIProvider):
    """Provider h√≠brido com fallback autom√°tico."""
    
    def __init__(
        self, 
        primary: AIProvider, 
        fallback: AIProvider,
        fallback_on_error: bool = True,
        fallback_on_timeout: float = 5.0
    ):
        self.primary = primary
        self.fallback = fallback
        self.fallback_on_error = fallback_on_error
        self.fallback_on_timeout = fallback_on_timeout
        self.primary_failures = 0
        self.circuit_open = False
    
    async def initialize(self) -> None:
        await asyncio.gather(
            self.primary.initialize(),
            self.fallback.initialize()
        )
        print("Hybrid Provider inicializado (primary + fallback)")
    
    async def shutdown(self) -> None:
        await asyncio.gather(
            self.primary.shutdown(),
            self.fallback.shutdown()
        )
    
    async def detect(
        self, 
        image_bytes: bytes, 
        detect_text: bool = False
    ) -> List[Dict[str, Any]]:
        # Circuit breaker: se muitas falhas, vai direto pro fallback
        if self.circuit_open:
            return await self.fallback.detect(image_bytes, detect_text)
        
        try:
            result = await asyncio.wait_for(
                self.primary.detect(image_bytes, detect_text),
                timeout=self.fallback_on_timeout
            )
            self.primary_failures = 0  # Reset on success
            return result
            
        except (asyncio.TimeoutError, Exception) as e:
            self.primary_failures += 1
            
            # Abrir circuit breaker ap√≥s 5 falhas consecutivas
            if self.primary_failures >= 5:
                self.circuit_open = True
                # Agendar fechamento do circuit em 60s
                asyncio.create_task(self._close_circuit_after(60))
            
            if self.fallback_on_error:
                print(f"Primary falhou ({e}), usando fallback")
                return await self.fallback.detect(image_bytes, detect_text)
            raise
    
    async def _close_circuit_after(self, seconds: float):
        await asyncio.sleep(seconds)
        self.circuit_open = False
        self.primary_failures = 0
        print("Circuit breaker fechado, primary reabilitado")
    
    async def detect_batch(
        self, 
        images: List[bytes]
    ) -> List[List[Dict[str, Any]]]:
        tasks = [self.detect(img, detect_text=True) for img in images]
        return await asyncio.gather(*tasks)
    
    async def health_check(self) -> Dict[str, Any]:
        primary_health = await self.primary.health_check()
        fallback_health = await self.fallback.health_check()
        
        return {
            "ok": primary_health["ok"] or fallback_health["ok"],
            "provider": "hybrid",
            "circuit_open": self.circuit_open,
            "primary": primary_health,
            "fallback": fallback_health
        }
```

**app/models/yolo_detector.py:**
```python
from ultralytics import YOLO
import numpy as np
import cv2
from io import BytesIO
from PIL import Image

class YOLODetector:
    def __init__(self, model_path: str, device: str = "cuda", confidence: float = 0.5):
        self.model = YOLO(model_path)
        self.model.to(device)
        self.confidence = confidence
        self.device = device
    
    def detect(self, image_bytes: bytes) -> list:
        """Detecta objetos na imagem."""
        # Converter bytes para numpy
        nparr = np.frombuffer(image_bytes, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        # Infer√™ncia
        results = self.model(img, conf=self.confidence, verbose=False)
        
        detections = []
        for result in results:
            boxes = result.boxes
            for box in boxes:
                detection = {
                    "class": result.names[int(box.cls)],
                    "confidence": float(box.conf),
                    "bbox": box.xyxy[0].tolist(),  # [x1, y1, x2, y2]
                }
                detections.append(detection)
        
        return detections
    
    def detect_batch(self, images: list[bytes]) -> list[list]:
        """Detecta objetos em m√∫ltiplas imagens."""
        # Converter todos para numpy
        imgs = []
        for img_bytes in images:
            nparr = np.frombuffer(img_bytes, np.uint8)
            img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
            imgs.append(img)
        
        # Batch inference
        results = self.model(imgs, conf=self.confidence, verbose=False)
        
        all_detections = []
        for result in results:
            detections = []
            for box in result.boxes:
                detection = {
                    "class": result.names[int(box.cls)],
                    "confidence": float(box.conf),
                    "bbox": box.xyxy[0].tolist(),
                }
                detections.append(detection)
            all_detections.append(detections)
        
        return all_detections
```

**app/workers/frame_grabber.py:**
```python
import asyncio
import httpx
from typing import Optional
import redis.asyncio as redis
from app.config import settings

class FrameGrabber:
    """Captura frames do MediaMTX para processamento."""
    
    def __init__(self):
        self.mediamtx_api = settings.MEDIAMTX_API_URL
        self.redis = redis.from_url(settings.REDIS_URL)
        self.http_client = httpx.AsyncClient(timeout=5.0)
    
    async def grab_frame(self, camera_id: int) -> Optional[bytes]:
        """Captura um frame de uma c√¢mera via MediaMTX API."""
        try:
            # Usar snapshot API do MediaMTX
            url = f"{self.mediamtx_api}/v3/paths/cam_{camera_id}/snapshot"
            response = await self.http_client.get(url)
            
            if response.status_code == 200:
                return response.content
            return None
            
        except Exception as e:
            print(f"Erro ao capturar frame da camera {camera_id}: {e}")
            return None
    
    async def grab_frames_batch(self, camera_ids: list[int]) -> dict[int, bytes]:
        """Captura frames de m√∫ltiplas c√¢meras em paralelo."""
        tasks = {cid: self.grab_frame(cid) for cid in camera_ids}
        results = await asyncio.gather(*tasks.values(), return_exceptions=True)
        
        frames = {}
        for cid, result in zip(tasks.keys(), results):
            if isinstance(result, bytes):
                frames[cid] = result
        
        return frames
    
    async def start_continuous_grabbing(self, camera_ids: list[int], fps: float = 1.0):
        """Inicia captura cont√≠nua de frames para processamento."""
        interval = 1.0 / fps
        
        while True:
            frames = await self.grab_frames_batch(camera_ids)
            
            # Publicar frames no Redis para workers processarem
            for camera_id, frame in frames.items():
                await self.redis.lpush(
                    f"frames:queue:{camera_id}",
                    frame
                )
                # Manter apenas √∫ltimos 10 frames na fila
                await self.redis.ltrim(f"frames:queue:{camera_id}", 0, 9)
            
            await asyncio.sleep(interval)
```

**docker-compose.yml (GPU Workers Locais):**
```yaml
ai-service-gpu:
  build:
    context: ./ai_service
    dockerfile: Dockerfile.gpu
  deploy:
    mode: replicated
    replicas: 2  # CONFIGUR√ÅVEL: Alta disponibilidade
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
  environment:
    - AI_BACKEND=gpu
    - CUDA_VISIBLE_DEVICES=0
    - GPU_DEVICE=cuda:0
    - YOLO_MODEL_PATH=/app/models/yolov8s.pt
    - LPR_MODEL_PATH=/app/models/custom_lpr.pt
    - CONFIDENCE_THRESHOLD=0.5
    - REDIS_URL=redis://redis:6379/0
    - MEDIAMTX_API_URL=http://mediamtx:9997
  volumes:
    - ./ai_service/models:/app/models:ro
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
    interval: 30s
    timeout: 10s
    retries: 3
  depends_on:
    - redis
    - mediamtx
  networks:
    - gt-vision-network
```

**docker-compose.aws.yml (EC2 Workers com Rekognition):**
```yaml
# Para usar em EC2: docker-compose -f docker-compose.yml -f docker-compose.aws.yml up
ai-service-aws:
  build:
    context: ./ai_service
    dockerfile: Dockerfile.cpu
  deploy:
    mode: replicated
    replicas: 4  # Mais r√©plicas (CPU √© barato)
  environment:
    - AI_BACKEND=aws
    - AWS_DEFAULT_REGION=${AWS_REGION:-us-east-1}
    - CONFIDENCE_THRESHOLD=0.5
    - REDIS_URL=redis://redis:6379/0
    - MEDIAMTX_API_URL=http://mediamtx:9997
    # Credenciais via IAM Role (recomendado) ou env vars
    # - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
    # - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
    interval: 30s
    timeout: 10s
    retries: 3
  depends_on:
    - redis
    - mediamtx
  networks:
    - gt-vision-network
```

**docker-compose.hybrid.yml (Modo H√≠brido):**
```yaml
# Para usar modo h√≠brido: docker-compose -f docker-compose.yml -f docker-compose.hybrid.yml up
ai-service-hybrid:
  build:
    context: ./ai_service
    dockerfile: Dockerfile.gpu  # Precisa de GPU para primary
  deploy:
    mode: replicated
    replicas: 2
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
  environment:
    - AI_BACKEND=hybrid
    - CUDA_VISIBLE_DEVICES=0
    - GPU_DEVICE=cuda:0
    - YOLO_MODEL_PATH=/app/models/yolov8s.pt
    - LPR_MODEL_PATH=/app/models/custom_lpr.pt
    - AWS_DEFAULT_REGION=${AWS_REGION:-us-east-1}
    - CONFIDENCE_THRESHOLD=0.5
    - FALLBACK_TIMEOUT=5.0
    - REDIS_URL=redis://redis:6379/0
    - MEDIAMTX_API_URL=http://mediamtx:9997
  volumes:
    - ./ai_service/models:/app/models:ro
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
    interval: 30s
    timeout: 10s
    retries: 3
  networks:
    - gt-vision-network
```

**Terraform para EC2 Workers (terraform/ec2-workers.tf):**
```hcl
# Vari√°veis
variable "aws_region" {
  default = "us-east-1"
}

variable "instance_type" {
  default = "c5.xlarge"  # 4 vCPU, 8GB RAM - bom para Rekognition
}

variable "min_workers" {
  default = 2
}

variable "max_workers" {
  default = 10
}

# IAM Role para Rekognition
resource "aws_iam_role" "ai_worker_role" {
  name = "gt-vision-ai-worker-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "ec2.amazonaws.com"
      }
    }]
  })
}

resource "aws_iam_role_policy" "rekognition_policy" {
  name = "rekognition-access"
  role = aws_iam_role.ai_worker_role.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "rekognition:DetectLabels",
          "rekognition:DetectText",
          "rekognition:DetectFaces",
          "rekognition:DescribeProjects"
        ]
        Resource = "*"
      }
    ]
  })
}

resource "aws_iam_instance_profile" "ai_worker_profile" {
  name = "gt-vision-ai-worker-profile"
  role = aws_iam_role.ai_worker_role.name
}

# Launch Template
resource "aws_launch_template" "ai_worker" {
  name_prefix   = "gt-vision-ai-worker-"
  image_id      = data.aws_ami.amazon_linux_2.id
  instance_type = var.instance_type

  iam_instance_profile {
    name = aws_iam_instance_profile.ai_worker_profile.name
  }

  user_data = base64encode(<<-EOF
    #!/bin/bash
    yum update -y
    yum install -y docker
    systemctl start docker
    systemctl enable docker
    
    # Login no ECR (se usando)
    # aws ecr get-login-password --region ${var.aws_region} | docker login --username AWS --password-stdin ${aws_account_id}.dkr.ecr.${var.aws_region}.amazonaws.com
    
    # Rodar AI Service
    docker run -d \
      --name ai-service \
      --restart always \
      -p 8000:8000 \
      -e AI_BACKEND=aws \
      -e AWS_DEFAULT_REGION=${var.aws_region} \
      -e REDIS_URL=${redis_url} \
      -e MEDIAMTX_API_URL=${mediamtx_url} \
      gt-vision/ai-service:latest
  EOF
  )

  tag_specifications {
    resource_type = "instance"
    tags = {
      Name = "gt-vision-ai-worker"
    }
  }
}

# Auto Scaling Group
resource "aws_autoscaling_group" "ai_workers" {
  name                = "gt-vision-ai-workers"
  desired_capacity    = var.min_workers
  min_size            = var.min_workers
  max_size            = var.max_workers
  vpc_zone_identifier = var.subnet_ids
  target_group_arns   = [aws_lb_target_group.ai_workers.arn]

  launch_template {
    id      = aws_launch_template.ai_worker.id
    version = "$Latest"
  }

  tag {
    key                 = "Name"
    value               = "gt-vision-ai-worker"
    propagate_at_launch = true
  }
}

# Auto Scaling Policy baseado em CPU
resource "aws_autoscaling_policy" "scale_up" {
  name                   = "scale-up"
  scaling_adjustment     = 2
  adjustment_type        = "ChangeInCapacity"
  cooldown               = 300
  autoscaling_group_name = aws_autoscaling_group.ai_workers.name
}

resource "aws_cloudwatch_metric_alarm" "high_cpu" {
  alarm_name          = "ai-workers-high-cpu"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = 2
  metric_name         = "CPUUtilization"
  namespace           = "AWS/EC2"
  period              = 60
  statistic           = "Average"
  threshold           = 70

  dimensions = {
    AutoScalingGroupName = aws_autoscaling_group.ai_workers.name
  }

  alarm_actions = [aws_autoscaling_policy.scale_up.arn]
}

# Load Balancer para AI Workers
resource "aws_lb" "ai_workers" {
  name               = "gt-vision-ai-lb"
  internal           = true
  load_balancer_type = "application"
  subnets            = var.subnet_ids
}

resource "aws_lb_target_group" "ai_workers" {
  name     = "gt-vision-ai-tg"
  port     = 8000
  protocol = "HTTP"
  vpc_id   = var.vpc_id

  health_check {
    path                = "/health"
    healthy_threshold   = 2
    unhealthy_threshold = 3
    timeout             = 5
    interval            = 30
  }
}

resource "aws_lb_listener" "ai_workers" {
  load_balancer_arn = aws_lb.ai_workers.arn
  port              = 80
  protocol          = "HTTP"

  default_action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.ai_workers.arn
  }
}

# Output
output "ai_lb_dns" {
  value = aws_lb.ai_workers.dns_name
}
```

**Valida√ß√£o:**
- [ ] AI Service rodando (GPU ou AWS mode)
- [ ] Endpoint /detect funcionando
- [ ] Endpoint /batch processando m√∫ltiplos frames
- [ ] M√©tricas expostas em /metrics
- [ ] Alta disponibilidade com 2+ r√©plicas
- [ ] **GPU Mode:** Lat√™ncia <100ms por frame
- [ ] **AWS Mode:** Lat√™ncia <300ms por frame
- [ ] **Hybrid Mode:** Fallback funcionando, circuit breaker testado
- [ ] **AWS:** Auto Scaling respondendo a carga
- [ ] **AWS:** IAM Role com permiss√µes corretas

---

### 2.1.1 Custos Estimados (AWS Rekognition)

| Opera√ß√£o | Pre√ßo (us-east-1) | 250 c√¢meras @ 1 FPS |
|----------|-------------------|---------------------|
| DetectLabels | $0.001/imagem | $21,600/m√™s |
| DetectText | $0.001/imagem | $21,600/m√™s |
| **Total** | - | **~$43,200/m√™s** |

**Otimiza√ß√µes de custo:**
- Reduzir FPS para 0.5 (1 frame a cada 2s): **$21,600/m√™s**
- Usar GPU local para c√¢meras cr√≠ticas, AWS para resto
- Implementar detec√ß√£o de movimento antes de enviar para IA
- Usar cache de resultados para frames similares

**Comparativo GPU Local vs AWS:**
| Aspecto | GPU Local (RTX 4090) | AWS Rekognition |
|---------|---------------------|-----------------|
| Custo inicial | ~$2,000/GPU | $0 |
| Custo mensal | ~$50 (energia) | ~$43,200 (250 cam) |
| Lat√™ncia | ~50ms | ~200ms |
| Escalabilidade | Limitada | Ilimitada |
| Manuten√ß√£o | Alta | Zero |
| Customiza√ß√£o | Total (modelos custom) | Limitada |

**Recomenda√ß√£o:** Modo h√≠brido - GPU para c√¢meras cr√≠ticas (LPR), AWS para resto.

---

### 2.2 Otimizar Ingest√£o de Detec√ß√µes (Gateway FastAPI)
**Objetivo:** Suportar >1000 detec√ß√µes/segundo sem perda.

**Tarefas:**
- [ ] Implementar batch insert no `gateway/main.py`
- [ ] Adicionar fila Redis para buffer (se DB lento)
- [ ] Usar connection pooling no PostgreSQL (PgBouncer)
- [ ] Adicionar √≠ndices no banco (camera_id, timestamp)
- [ ] Implementar rate limiting por c√¢mera (evitar spam)
- [ ] Integrar com AI Service para receber detec√ß√µes

**Otimiza√ß√£o de ingest√£o:**
```python
from fastapi import FastAPI, BackgroundTasks
from redis import asyncio as aioredis
import asyncpg
from datetime import datetime
import json

# CONFIGUR√ÅVEL: BATCH_SIZE para ajustar throughput vs lat√™ncia
BATCH_SIZE = 100
BATCH_TIMEOUT = 1.0  # segundos

app = FastAPI()
redis_client = None
db_pool = None
detection_buffer = []

@app.on_event("startup")
async def startup():
    global redis_client, db_pool
    redis_client = await aioredis.from_url("redis://redis:6379/0")
    db_pool = await asyncpg.create_pool(
        dsn="postgresql://user:pass@pgbouncer:6432/gtvision",
        min_size=10,
        max_size=50
    )

@app.post("/fast-api/ingest/detection")
async def ingest_detection(detection: dict, background_tasks: BackgroundTasks):
    """Recebe detec√ß√µes do AI Service."""
    # Adicionar timestamp
    detection['received_at'] = datetime.utcnow().isoformat()
    
    # Buffer em mem√≥ria
    detection_buffer.append(detection)
    
    if len(detection_buffer) >= BATCH_SIZE:
        # Flush ass√≠ncrono
        batch = detection_buffer.copy()
        detection_buffer.clear()
        background_tasks.add_task(flush_to_db, batch)
    
    # Publicar para WebSocket subscribers
    await redis_client.publish(
        f"detections:{detection['camera_id']}",
        json.dumps(detection)
    )
    
    return {"status": "queued", "buffer_size": len(detection_buffer)}

async def flush_to_db(batch: list):
    """Batch insert no PostgreSQL."""
    async with db_pool.acquire() as conn:
        await conn.executemany(
            """
            INSERT INTO detections (camera_id, class, confidence, bbox, plate, timestamp)
            VALUES ($1, $2, $3, $4, $5, $6)
            """,
            [
                (d['camera_id'], d['class'], d['confidence'], 
                 json.dumps(d['bbox']), d.get('plate'), d['timestamp'])
                for d in batch
            ]
        )
```

**Valida√ß√£o:**
- [ ] Teste de carga: 1000 req/s com Locust
- [ ] Lat√™ncia p95 <50ms
- [ ] Zero perda de dados

---

### 2.3 Implementar PgBouncer (Connection Pooling)
**Objetivo:** Reduzir overhead de conex√µes ao PostgreSQL.

**Tarefas:**
- [ ] Adicionar servi√ßo `pgbouncer` ao `docker-compose.yml`
- [ ] Configurar pool de 100 conex√µes
- [ ] Apontar Django e Gateway para PgBouncer (porta 6432)
- [ ] Configurar modo `transaction` (melhor performance)

**docker-compose.yml:**
```yaml
pgbouncer:
  image: pgbouncer/pgbouncer:latest
  environment:
    - DATABASES_HOST=postgres_db
    - DATABASES_PORT=5432
    - DATABASES_USER=${POSTGRES_USER}
    - DATABASES_PASSWORD=${POSTGRES_PASSWORD}
    - DATABASES_DBNAME=${POSTGRES_DB}
    - PGBOUNCER_POOL_MODE=transaction
    - PGBOUNCER_MAX_CLIENT_CONN=1000    # CONFIGUR√ÅVEL
    - PGBOUNCER_DEFAULT_POOL_SIZE=25    # CONFIGUR√ÅVEL
  ports:
    - "6432:6432"
  depends_on:
    - postgres_db
  healthcheck:
    test: ["CMD", "pg_isready", "-h", "localhost", "-p", "6432"]
    interval: 10s
    timeout: 5s
    retries: 5
```

**Valida√ß√£o:**
- [ ] Django conecta via PgBouncer
- [ ] Verificar `SHOW POOLS;` no PgBouncer
- [ ] Lat√™ncia de queries mantida ou melhorada

---

### 2.4 Implementar MinIO (Object Storage)
**Objetivo:** Armazenar frames, grava√ß√µes e evid√™ncias de forma escal√°vel.

**Tarefas:**
- [ ] Adicionar MinIO ao `docker-compose.yml`
- [ ] Configurar buckets: frames, recordings, evidence
- [ ] Configurar lifecycle policy (reten√ß√£o 7 dias para frames)
- [ ] Integrar AI Service para salvar frames processados
- [ ] Configurar replica√ß√£o (opcional, para HA)

**docker-compose.yml:**
```yaml
minio:
  image: minio/minio:latest
  command: server /data --console-address ":9001"
  environment:
    - MINIO_ROOT_USER=${MINIO_ROOT_USER}
    - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD}
  volumes:
    - minio_data:/data
  ports:
    - "9000:9000"   # API
    - "9001:9001"   # Console
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
    interval: 30s
    timeout: 10s
    retries: 3
```

**Configura√ß√£o de buckets (script de inicializa√ß√£o):**
```bash
#!/bin/bash
# scripts/init-minio.sh

mc alias set myminio http://minio:9000 ${MINIO_ROOT_USER} ${MINIO_ROOT_PASSWORD}

# Criar buckets
mc mb myminio/frames --ignore-existing
mc mb myminio/recordings --ignore-existing
mc mb myminio/evidence --ignore-existing

# Lifecycle policy - deletar frames ap√≥s 7 dias
mc ilm rule add myminio/frames --expire-days 7

# Lifecycle policy - deletar recordings ap√≥s 30 dias
mc ilm rule add myminio/recordings --expire-days 30

echo "MinIO buckets configurados!"
```

**Valida√ß√£o:**
- [ ] MinIO Console acess√≠vel (:9001)
- [ ] Buckets criados
- [ ] Lifecycle policies funcionando
- [ ] AI Service salvando frames

---

### 2.5 Otimizar Queries Django (Gargalos Conhecidos)
**Objetivo:** Reduzir lat√™ncia de listagens e dashboards.

**Tarefas:**
- [ ] Adicionar `select_related()` e `prefetch_related()` em ViewSets
- [ ] Criar √≠ndices compostos no PostgreSQL
- [ ] Usar `only()` e `defer()` para reduzir campos carregados
- [ ] Implementar pagina√ß√£o cursor-based para listas grandes
- [ ] Cachear queries pesadas no Redis (TTL 5s)

**√çndices cr√≠ticos:**
```sql
-- CONFIGUR√ÅVEL: Ajustar conforme queries mais frequentes
CREATE INDEX CONCURRENTLY idx_deteccoes_camera_ts 
  ON deteccoes(camera_id, timestamp DESC);

CREATE INDEX CONCURRENTLY idx_deteccoes_ts 
  ON deteccoes(timestamp DESC) 
  WHERE timestamp > NOW() - INTERVAL '7 days';

CREATE INDEX CONCURRENTLY idx_cameras_ativa 
  ON cameras(ativa) 
  WHERE ativa = true;
```

**Valida√ß√£o:**
- [ ] `EXPLAIN ANALYZE` em queries lentas
- [ ] Lat√™ncia de listagem <100ms
- [ ] Dashboard carrega em <500ms

---

## üìã FASE 3: FRONTEND (Semana 3)

### 3.1 Otimizar Bundle Size (Code Splitting)
**Objetivo:** Reduzir bundle de >2MB para <500KB (gzipped).

**Tarefas:**
- [ ] Analisar bundle com `npm run build -- --analyze`
- [ ] Implementar lazy loading de rotas
- [ ] Remover bibliotecas n√£o utilizadas
- [ ] Substituir bibliotecas pesadas por alternativas leves
- [ ] Configurar tree-shaking no Vite

**Otimiza√ß√µes:**
```typescript
// Lazy loading de p√°ginas
const Dashboard = lazy(() => import('./pages/Dashboard'));
const Cameras = lazy(() => import('./pages/Cameras'));

// Remover libs pesadas
// ‚ùå moment.js (500KB) ‚Üí ‚úÖ date-fns (10KB)
// ‚ùå lodash completo ‚Üí ‚úÖ lodash-es (tree-shakeable)
```

**Valida√ß√£o:**
- [ ] Bundle principal <200KB (gzipped)
- [ ] Chunks de rotas <100KB cada
- [ ] Lighthouse score >90

---

### 3.2 Otimizar Player de V√≠deo (HLS.js)
**Objetivo:** Player leve com overlay de detec√ß√µes via Canvas.

**Tarefas:**
- [ ] Usar HLS.js nativo (sem wrappers pesados)
- [ ] Implementar Canvas overlay para bounding boxes
- [ ] Adicionar fallback para WebRTC (baixa lat√™ncia)
- [ ] Implementar lazy loading de players (s√≥ carrega quando vis√≠vel)
- [ ] Otimizar re-renders com `React.memo()`

**Player otimizado:**
```typescript
// CONFIGUR√ÅVEL: HLS_BUFFER_SIZE para ajustar lat√™ncia
const HLS_CONFIG = {
  maxBufferLength: 10,        // CONFIGUR√ÅVEL: Menor = menos lat√™ncia
  maxMaxBufferLength: 20,
  liveSyncDuration: 3,
};

const VideoPlayer = React.memo(({ cameraId }) => {
  const videoRef = useRef<HTMLVideoElement>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);
  
  // Renderiza bounding boxes no Canvas (n√£o no DOM)
  const drawDetections = useCallback((detections) => {
    const ctx = canvasRef.current?.getContext('2d');
    // ... desenha ret√¢ngulos
  }, []);
  
  return (
    <>
      <video ref={videoRef} />
      <canvas ref={canvasRef} />
    </>
  );
});
```

**Valida√ß√£o:**
- [ ] Player carrega em <1s
- [ ] Overlay de detec√ß√µes sem lag
- [ ] Suporta 16 streams simult√¢neos sem travar

---

### 3.3 Implementar Virtual Scrolling (Listas Grandes)
**Objetivo:** Renderizar apenas itens vis√≠veis em listas de c√¢meras/detec√ß√µes.

**Tarefas:**
- [ ] Instalar `@tanstack/react-virtual`
- [ ] Implementar em lista de c√¢meras
- [ ] Implementar em lista de detec√ß√µes
- [ ] Adicionar skeleton loading

**Valida√ß√£o:**
- [ ] Lista de 1000 itens renderiza instantaneamente
- [ ] Scroll suave (60fps)

---

## üìã FASE 4: OBSERVABILIDADE COMPLETA (Semana 4)

### 4.1 Implementar Stack de Observabilidade Completa
**Objetivo:** M√©tricas, Logs e Tracing centralizados.

**Componentes:**
- **Prometheus**: M√©tricas
- **Grafana**: Dashboards
- **Loki**: Logs agregados
- **Jaeger**: Distributed tracing
- **Alertmanager**: Alertas ‚Üí PagerDuty/Slack

**Tarefas:**
- [ ] Adicionar stack completa ao `docker-compose.yml`
- [ ] Configurar exporters: node, postgres, redis, nginx
- [ ] Configurar Promtail para coletar logs
- [ ] Configurar Jaeger para tracing
- [ ] Criar dashboards Grafana
- [ ] Configurar alertas no Alertmanager

**docker-compose.yml:**
```yaml
# Prometheus
prometheus:
  image: prom/prometheus:latest
  volumes:
    - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    - prometheus_data:/prometheus
  command:
    - '--config.file=/etc/prometheus/prometheus.yml'
    - '--storage.tsdb.path=/prometheus'
    - '--storage.tsdb.retention.time=15d'
  ports:
    - "9090:9090"

# Grafana
grafana:
  image: grafana/grafana:latest
  environment:
    - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
    - GF_USERS_ALLOW_SIGN_UP=false
  volumes:
    - grafana_data:/var/lib/grafana
    - ./grafana/provisioning:/etc/grafana/provisioning
    - ./grafana/dashboards:/var/lib/grafana/dashboards
  ports:
    - "3001:3000"
  depends_on:
    - prometheus
    - loki

# Loki (Logs)
loki:
  image: grafana/loki:latest
  volumes:
    - ./loki/loki-config.yml:/etc/loki/local-config.yaml
    - loki_data:/loki
  ports:
    - "3100:3100"
  command: -config.file=/etc/loki/local-config.yaml

# Promtail (Log collector)
promtail:
  image: grafana/promtail:latest
  volumes:
    - ./promtail/promtail-config.yml:/etc/promtail/config.yml
    - /var/log:/var/log:ro
    - /var/lib/docker/containers:/var/lib/docker/containers:ro
  command: -config.file=/etc/promtail/config.yml
  depends_on:
    - loki

# Jaeger (Tracing)
jaeger:
  image: jaegertracing/all-in-one:latest
  environment:
    - COLLECTOR_OTLP_ENABLED=true
  ports:
    - "16686:16686"  # UI
    - "4317:4317"    # OTLP gRPC
    - "4318:4318"    # OTLP HTTP

# Alertmanager
alertmanager:
  image: prom/alertmanager:latest
  volumes:
    - ./alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml
  ports:
    - "9093:9093"
  command:
    - '--config.file=/etc/alertmanager/alertmanager.yml'
```

**prometheus/prometheus.yml:**
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

alerting:
  alertmanagers:
    - static_configs:
        - targets: ['alertmanager:9093']

rule_files:
  - '/etc/prometheus/rules/*.yml'

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'ai-service'
    static_configs:
      - targets: ['ai-service:8000']
    metrics_path: /metrics

  - job_name: 'gateway'
    static_configs:
      - targets: ['gateway:8001']

  - job_name: 'django'
    static_configs:
      - targets: ['django:8000']
    metrics_path: /metrics

  - job_name: 'mediamtx'
    static_configs:
      - targets: ['mediamtx:9998']

  - job_name: 'kong'
    static_configs:
      - targets: ['kong:8001']
    metrics_path: /metrics

  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']

  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
```

**alertmanager/alertmanager.yml:**
```yaml
global:
  slack_api_url: '${SLACK_WEBHOOK_URL}'

route:
  group_by: ['alertname', 'severity']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'slack-notifications'
  routes:
    - match:
        severity: critical
      receiver: 'pagerduty-critical'

receivers:
  - name: 'slack-notifications'
    slack_configs:
      - channel: '#gt-vision-alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  - name: 'pagerduty-critical'
    pagerduty_configs:
      - service_key: '${PAGERDUTY_SERVICE_KEY}'
        severity: critical
```

**Valida√ß√£o:**
- [ ] Prometheus coletando m√©tricas de todos servi√ßos
- [ ] Grafana com dashboards funcionais
- [ ] Loki agregando logs
- [ ] Jaeger mostrando traces
- [ ] Alertas chegando no Slack

---

### 4.2 Testes de Carga (Locust)
**Objetivo:** Validar que sistema suporta 250 c√¢meras + 100 usu√°rios.

**Tarefas:**
- [ ] Criar `tests/load/api_load.py` (Locust)
- [ ] Simular 100 usu√°rios acessando dashboard
- [ ] Simular 1000 detec√ß√µes/segundo
- [ ] Simular 50 streams simult√¢neos
- [ ] Medir lat√™ncia p95, p99
- [ ] Identificar gargalos

**Cen√°rios de teste:**
```python
# tests/load/locustfile.py
from locust import HttpUser, task, between, events
import random

class APIUser(HttpUser):
    wait_time = between(1, 3)
    
    @task(3)
    def view_dashboard(self):
        self.client.get("/api/analytics/dashboard/")
    
    @task(2)
    def list_cameras(self):
        self.client.get("/api/cameras/")
    
    @task(1)
    def view_detections(self):
        camera_id = random.randint(1, 250)
        self.client.get(f"/api/deteccoes/?camera_id={camera_id}")

class AIUser(HttpUser):
    """Simula carga no servi√ßo de IA."""
    wait_time = between(0.5, 1)
    
    @task
    def detect_frame(self):
        with open("test_frame.jpg", "rb") as f:
            self.client.post(
                "/detect/frame",
                files={"file": f},
                data={"camera_id": random.randint(1, 250)}
            )
```

**Valida√ß√£o:**
- [ ] API: p95 <100ms, p99 <200ms
- [ ] AI Service: p95 <100ms por frame
- [ ] Ingest√£o: >1000 req/s sem erros
- [ ] V√≠deo: lat√™ncia <3s (HLS)
- [ ] Zero crashes ou timeouts

---

### 4.3 Testes de Resili√™ncia
**Objetivo:** Sistema se recupera de falhas automaticamente.

**Tarefas:**
- [ ] Testar queda de PostgreSQL (failover para r√©plica)
- [ ] Testar queda de Redis (reconex√£o autom√°tica)
- [ ] Testar queda de MediaMTX (reconex√£o de c√¢meras)
- [ ] Testar queda de AI Service (load balancer redireciona)
- [ ] Testar queda de c√¢mera (health check detecta)
- [ ] Testar sobrecarga (rate limiting funciona)

**Valida√ß√£o:**
- [ ] Downtime <30s em falhas de componentes
- [ ] Dados n√£o s√£o perdidos
- [ ] Alertas s√£o disparados
- [ ] AI Service mant√©m disponibilidade com r√©plicas

---

## üìä CHECKLIST FINAL (MVP Ready)

### Performance
- [ ] API: p95 <100ms
- [ ] AI Service (GPU): p95 <100ms por frame
- [ ] AI Service (AWS): p95 <300ms por frame
- [ ] V√≠deo HLS: lat√™ncia <3s
- [ ] V√≠deo WebRTC: lat√™ncia <500ms
- [ ] Ingest√£o: >1000 detec√ß√µes/s
- [ ] Frontend: Lighthouse >90

### Escala
- [ ] 250 c√¢meras simult√¢neas est√°veis
- [ ] 100 usu√°rios concorrentes
- [ ] 50 streams simult√¢neos por usu√°rio
- [ ] AI Service processando 250 FPS (1 frame/s por c√¢mera)
- [ ] AWS Auto Scaling funcionando (2-10 inst√¢ncias)

### Recursos
- [ ] CPU <70% (carga normal)
- [ ] RAM <80% (carga normal)
- [ ] GPU <80% (AI Service GPU mode)
- [ ] Disco <85%
- [ ] Rede <80% capacidade
- [ ] **AWS:** Custo dentro do or√ßamento

### Alta Disponibilidade
- [ ] AI Service com 2+ r√©plicas (GPU ou EC2)
- [ ] **Modo H√≠brido:** Fallback GPU ‚Üí AWS funcionando
- [ ] **Circuit Breaker:** Testado e funcionando
- [ ] Kong com health checks
- [ ] PostgreSQL com r√©plica de leitura
- [ ] Redis Cluster configurado
- [ ] Failover autom√°tico funcionando

### Observabilidade
- [ ] Prometheus coletando m√©tricas (incluindo AWS)
- [ ] Grafana com dashboards
- [ ] Loki agregando logs
- [ ] Jaeger com tracing
- [ ] Alertas configurados (Slack + PagerDuty)
- [ ] **AWS:** CloudWatch Alarms configurados
- [ ] **AWS:** Billing Alerts configurados

### Seguran√ßa
- [ ] HTTPS em produ√ß√£o (Kong SSL termination)
- [ ] JWT via Keycloak
- [ ] Rate limiting ativo (Kong)
- [ ] Senhas criptografadas
- [ ] WAF configurado (CloudFlare)
- [ ] **AWS:** IAM Roles com least privilege
- [ ] **AWS:** VPC Endpoints para Rekognition

---

## üîß CONFIGURA√á√ïES PARA AJUSTE FINO

### HAProxy
```
# CONFIGUR√ÅVEL: Timeouts
timeout connect 5s
timeout client 30s
timeout server 30s
timeout tunnel 1h    # Para WebRTC/WebSocket
```

### Kong
```yaml
# CONFIGUR√ÅVEL: Rate Limiting
plugins:
  - name: rate-limiting
    config:
      minute: 100
      hour: 1000
      policy: redis
      redis_host: redis
```

### AI Service
```python
# CONFIGUR√ÅVEL: Performance (comum)
CONFIDENCE_THRESHOLD = 0.5   # Threshold de detec√ß√£o (0.0-1.0)
BATCH_SIZE = 8               # Frames por batch

# GPU Mode
GPU_DEVICE = "cuda:0"        # Qual GPU usar
GPU_MEMORY_FRACTION = 0.8    # % da GPU a usar
WORKERS_PER_GPU = 2          # Workers por GPU
YOLO_MODEL = "yolov8s.pt"    # n=nano, s=small, m=medium, l=large

# AWS Mode
AWS_REGION = "us-east-1"     # Regi√£o (us-east-1 √© mais barato)
AWS_MAX_CONCURRENT = 50      # Requests simult√¢neas ao Rekognition
AWS_RETRY_ATTEMPTS = 3       # Tentativas em caso de erro

# Hybrid Mode
FALLBACK_TIMEOUT = 5.0       # Timeout para fallback (segundos)
CIRCUIT_BREAKER_THRESHOLD = 5 # Falhas para abrir circuit
CIRCUIT_BREAKER_RESET = 60   # Segundos para fechar circuit
```

### AWS Auto Scaling
```hcl
# CONFIGUR√ÅVEL: Scaling
min_workers = 2              # M√≠nimo de inst√¢ncias
max_workers = 10             # M√°ximo de inst√¢ncias
scale_up_threshold = 70      # CPU % para scale up
scale_down_threshold = 30    # CPU % para scale down
cooldown_period = 300        # Segundos entre scaling
instance_type = "c5.xlarge"  # 4 vCPU, 8GB RAM
```

### MediaMTX
```yaml
# CONFIGUR√ÅVEL: Performance vs Lat√™ncia
hlsSegmentDuration: 2s    # 1s=baixa lat√™ncia, 4s=menos CPU
writeQueueSize: 1024      # Aumentar se drops de frames
```

### PostgreSQL
```sql
-- CONFIGUR√ÅVEL: Tuning para 250 c√¢meras
shared_buffers = 2GB
effective_cache_size = 6GB
work_mem = 16MB
maintenance_work_mem = 512MB
max_connections = 200
```

### Redis
```
# CONFIGUR√ÅVEL: Mem√≥ria
maxmemory 1gb
maxmemory-policy allkeys-lru
```

---

## üìÖ CRONOGRAMA ATUALIZADO

| Semana | Fase | Entregas |
|--------|------|----------|
| 1 | Infra Core | ~~HAProxy~~‚úÖ, ~~MediaMTX~~‚úÖ, ~~Nginx~~‚úÖ, ~~Kong~~‚úÖ, Keycloak |
| 2 | Backend + IA | **AI Service (FastAPI + YOLO + TF)**, PgBouncer, MinIO |
| 3 | Frontend | Bundle otimizado, Player leve, Virtual scroll |
| 4 | Observabilidade | Prometheus, Grafana, Loki, Jaeger, Alertmanager |

**Data de entrega:** Final de Janeiro 2025

---

## üìù SESS√ÉO 16/12/2024 - Corre√ß√µes e Melhorias

### ‚úÖ Problemas Corrigidos

#### 1. Django Admin sem CSS
**Problema:** Admin Django aparecia sem formata√ß√£o (HTML puro).

**Causa:** Arquivos est√°ticos n√£o estavam sendo servidos corretamente atrav√©s do Kong/HAProxy.

**Solu√ß√£o:**
- Adicionada rota `/static` e `/media` no Kong (`kong/kong.yml`)
- Kong agora roteia arquivos est√°ticos para Nginx
- Configurado `CSRF_TRUSTED_ORIGINS` no Django para aceitar requests via proxy
- Criados scripts de corre√ß√£o: `fix-css.bat`, `check-static.bat`

**Arquivos modificados:**
- `kong/kong.yml` - Nova rota `django-static`
- `backend/config/settings.py` - CSRF origins + cookies config
- `fix-css.bat` - Script autom√°tico de corre√ß√£o
- `check-static.bat` - Script de diagn√≥stico
- `TROUBLESHOOTING-CSS.md` - Guia completo
- `README-CSS-FIX.md` - Guia r√°pido

#### 2. Erro CSRF 403 no Django Admin
**Problema:** Login no admin retornava erro "Verifica√ß√£o CSRF falhou".

**Causa:** Django bloqueando requests vindos atrav√©s de Kong/HAProxy.

**Solu√ß√£o:**
```python
# settings.py
CSRF_TRUSTED_ORIGINS = [
    "http://localhost",
    "http://localhost:8000",
    "http://127.0.0.1",
    # ... outras origens
]

CSRF_USE_SESSIONS = False
CSRF_COOKIE_HTTPONLY = False
CSRF_COOKIE_SAMESITE = 'Lax'

if DEBUG:
    CSRF_COOKIE_SECURE = False
    SESSION_COOKIE_SECURE = False
```

#### 3. Erro 503 no HAProxy
**Problema:** HAProxy retornando 503 ao acessar `/admin/`.

**Causa:** HAProxy roteando para Kong, mas Kong n√£o tinha rota configurada.

**Solu√ß√£o:**
- Verificado que Kong j√° tinha rota `/admin` configurada
- Problema era que backend estava reiniciando (healthcheck)
- Aguardar backend ficar "healthy" antes de acessar

### üìÅ Arquivos Criados

1. **fix-css.bat** - Script autom√°tico de corre√ß√£o de CSS
2. **check-static.bat** - Script de diagn√≥stico de arquivos est√°ticos
3. **open-admin.bat** - Script para abrir admin pela porta correta
4. **diagnose.bat** - Script de diagn√≥stico completo do sistema
5. **TROUBLESHOOTING-CSS.md** - Guia completo de troubleshooting
6. **README-CSS-FIX.md** - Guia r√°pido de uso dos scripts

### üîß Configura√ß√µes Validadas

#### Arquitetura de Fluxo Confirmada
```
Fluxo de Usu√°rio (React App / API):
Cloudflare (WAF/DDoS/SSL) ‚Üí HAProxy ‚Üí Kong ‚Üí Backend Django

Fluxo de V√≠deo (Playback):
Cloudflare (Cache HLS) ‚Üí HAProxy ‚Üí MediaMTX

Fluxo de C√¢mera (Ingest√£o):
C√¢mera (RTSP) ‚Üí HAProxy (TCP Balance) ‚Üí MediaMTX

Fluxo de Arquivos Est√°ticos:
Cloudflare ‚Üí HAProxy ‚Üí Kong ‚Üí Nginx
```

#### HAProxy Configurado
- ‚úÖ Roteamento de `/admin/` para Kong
- ‚úÖ Roteamento de `/api/` para Kong
- ‚úÖ Roteamento de `/static/` para Kong ‚Üí Nginx
- ‚úÖ Roteamento de v√≠deo direto para MediaMTX (bypass Kong)
- ‚úÖ Sticky sessions para WebRTC
- ‚úÖ Health checks funcionando

#### Kong Configurado
- ‚úÖ Rota `/api` ‚Üí Backend Django
- ‚úÖ Rota `/admin` ‚Üí Backend Django
- ‚úÖ Rota `/static` ‚Üí Nginx (NOVO)
- ‚úÖ Rota `/media` ‚Üí Nginx (NOVO)
- ‚úÖ Rota `/fast-api` ‚Üí Gateway FastAPI
- ‚úÖ Rate limiting configurado
- ‚úÖ CORS configurado
- ‚úÖ Prometheus metrics habilitadas

### üéØ Pr√≥ximas A√ß√µes

1. **Testar fluxo completo:**
   - [ ] Login no admin funcionando
   - [ ] CSS carregando corretamente
   - [ ] API acess√≠vel via Kong
   - [ ] Arquivos est√°ticos servidos

2. **Commit das mudan√ßas:**
   ```bash
   git add .
   git commit -m "fix: corrige CSS do Django Admin e CSRF via Kong/HAProxy
   
   - Adiciona rota de static files no Kong
   - Configura CSRF_TRUSTED_ORIGINS para proxies
   - Cria scripts de diagn√≥stico e corre√ß√£o
   - Adiciona documenta√ß√£o de troubleshooting"
   ```

3. **Continuar com Fase 1.5:** Implementar Keycloak

### üìä Status Atual

**Infraestrutura Core (Fase 1):**
- [x] HAProxy (Load Balancer)
- [x] MediaMTX (Streaming)
- [x] Nginx (Arquivos Est√°ticos)
- [x] Kong (API Gateway)
- [x] Django Admin acess√≠vel e funcional
- [ ] Keycloak (Auth/Identity) ‚Üê PR√ìXIMO

**Valida√ß√µes Pendentes:**
- [ ] Teste de carga com 50 c√¢meras
- [ ] Teste de failover do backend
- [ ] Teste de rate limiting do Kong
- [ ] Monitoramento com Prometheus

---

## üö® RISCOS E MITIGA√á√ïES

| Risco | Impacto | Mitiga√ß√£o |
|-------|---------|-----------|
| GPU n√£o dispon√≠vel | Cr√≠tico | Fallback para AWS Rekognition (modo h√≠brido) |
| AI Service sobrecarregado | Alto | Auto-scaling (EC2), rate limiting, batch processing |
| **Custo AWS explode** | **Alto** | Monitorar billing, alertas de custo, reduzir FPS, detec√ß√£o de movimento |
| **Lat√™ncia AWS alta** | M√©dio | VPC Endpoints, regi√£o mais pr√≥xima, cache de resultados |
| **AWS throttling** | M√©dio | Exponential backoff, aumentar service quotas |
| MediaMTX n√£o aguenta 250 c√¢meras | Alto | Testar com 50, 100, 150 incrementalmente |
| Disco enche r√°pido (8TB/semana) | Alto | MinIO lifecycle, limpeza autom√°tica, alertas |
| Lat√™ncia de rede alta | M√©dio | CDN para v√≠deo, compress√£o |
| PostgreSQL lento | Alto | PgBouncer, √≠ndices, r√©plicas de leitura |
| Frontend pesado | M√©dio | Code splitting, lazy loading |

---

## üéØ PR√ìXIMOS PASSOS

1. **Implementar Kong API Gateway** (Fase 1.4)
2. **Criar AI Service com FastAPI** (Fase 2.1) ‚Üê PRIORIDADE ALTA
   - Come√ßar com GPU Provider (desenvolvimento local)
   - Adicionar AWS Provider
   - Implementar Hybrid Provider
3. **Configurar Keycloak** (Fase 1.5)
4. **Implementar MinIO** (Fase 2.4)
5. **Configurar stack de observabilidade** (Fase 4.1)
6. **Deploy AWS** (ap√≥s valida√ß√£o local)
   - Criar infraestrutura Terraform
   - Configurar Auto Scaling
   - Testar failover h√≠brido