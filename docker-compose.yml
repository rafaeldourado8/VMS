version: '3.8'

services:
  backend:
    build:
      context: .
      dockerfile: ./backend/Dockerfile
    # Para testes com docker-compose, use 1 réplica. Em produção, use orquestrador + leader migration.
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
    volumes:
      - ./backend/logs:/app/logs
      - backend_static:/app/staticfiles
      - backend_media:/app/media
      # Permite que a API liste/veja os arquivos brutos (Somente Leitura)
      - mediamtx_recordings:/recordings:ro
    env_file:
      - ./.env
    environment:
      # Conecta ao PgBouncer
      DB_HOST: pgbouncer
      DB_PORT: 6432
      REDIS_HOST: redis_cache
      REDIS_PORT: 6379
      AI_SERVICE_URL: ""
      MEDIAMTX_API_URL: http://mediamtx:9997
      CELERY_BROKER_URL: amqp://${RABBITMQ_USER}:${RABBITMQ_PASS}@rabbitmq:5672//
      CELERY_RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres_db:5432/${POSTGRES_DB}
    expose:
      - "8000"
    depends_on:
      postgres_db:
        condition: service_healthy
      pgbouncer:
        condition: service_started
      redis_cache:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    networks:
      - gtvision_network
    restart: unless-stopped
    # Simplificar startup: NÃO executar migrate/collectstatic automaticamente em múltiplas réplicas.
    # Execute migrations manualmente antes de escalar ou implemente um leader-only migration job.
    command: >
      sh -c "gunicorn config.wsgi:application --bind 0.0.0.0:8000 --workers 4 --timeout 180"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/admin/login/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  backend_worker:
    build:
      context: .
      dockerfile: ./backend/Dockerfile
    container_name: gtvision_backend_worker
    command: celery -A config worker --loglevel=info
    volumes:
      - ./backend/logs:/app/logs
      # ADICIONADO: O Worker precisa ver os arquivos para fazer recortes (FFmpeg)
      - mediamtx_recordings:/recordings:ro
    env_file:
      - ./.env
    environment:
      DB_HOST: pgbouncer
      DB_PORT: 6432
      REDIS_HOST: redis_cache
      REDIS_PORT: 6379
      AI_SERVICE_URL: ""
      MEDIAMTX_API_URL: http://mediamtx:9997
      CELERY_BROKER_URL: amqp://${RABBITMQ_USER}:${RABBITMQ_PASS}@rabbitmq:5672//
      CELERY_RESULT_BACKEND: db+postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres_db:5432/${POSTGRES_DB}
    depends_on:
      postgres_db:
        condition: service_healthy
      pgbouncer:
        condition: service_started
      redis_cache:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
    networks:
      - gtvision_network
    restart: unless-stopped

  # --- GATEWAY (CACHE & PROXY) ---
  gateway:
    build: ./gateway
    container_name: gtvision_gateway
    restart: unless-stopped
    environment:
      - DJANGO_BACKEND_URL=http://backend:8000
      - REDIS_URL=redis://redis_cache:6379/1
    depends_on:
      backend:
        condition: service_started
      redis_cache:
        condition: service_healthy
    networks:
      - gtvision_network

  # --- LOCUST (TESTES DE CARGA) ---
  locust:
    image: locustio/locust
    container_name: gtvision_locust
    ports:
      - "8089:8089"
    volumes:
      - ./backend/locustfile.py:/mnt/locust/locustfile.py
    environment:
      - LOCUST_HOST=http://nginx:80
    networks:
      - gtvision_network

  pgbouncer:
    image: edoburu/pgbouncer
    container_name: gtvision_pgbouncer
    environment:
      - DB_USER=${POSTGRES_USER}
      - DB_PASSWORD=${POSTGRES_PASSWORD}
      - DB_HOST=postgres_db
      - DB_NAME=${POSTGRES_DB}
      - POOL_MODE=transaction
      - MAX_CLIENT_CONN=1000
      - DEFAULT_POOL_SIZE=20
      - ADMIN_USERS=${POSTGRES_USER}
      - AUTH_TYPE=trust
      - AUTH_USER=${POSTGRES_USER}
      - LISTEN_ADDR=*
      - LISTEN_PORT=6432
    ports:
      - "6432:6432"
    depends_on:
      postgres_db:
        condition: service_healthy
    networks:
      - gtvision_network
    restart: unless-stopped

  rabbitmq:
    image: rabbitmq:3.13-management-alpine
    container_name: gtvision_rabbitmq
    hostname: rabbitmq
    env_file:
      - ./.env
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER}
      RABBITMQ_DEFAULT_VHOST: /
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASS}
    ports:
      - "5672:5672"
      - "15672:15672"
    networks:
      - gtvision_network
    restart: unless-stopped
    # CORREÇÃO AQUI: Aumentado tempos para evitar morte prematura do container
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "check_running"]
      interval: 10s
      timeout: 10s
      retries: 20
      start_period: 120s

  mediamtx:
    image: bluenviron/mediamtx:latest-ffmpeg
    container_name: gtvision_mediamtx
    ports:
      - "8554:8554"
      - "1935:1935"
      - "8888:8888"
      - "8889:8889"
      - "8889:8889/udp"
      - "8189:8189/udp"
      - "9997:9997"
      - "9996:9996" # Porta de Playback (HLS Histórico)
    environment:
      - MEDIAMTX_API_PASS=${MEDIAMTX_API_PASS}
    volumes:
      - ./mediamtx.yml:/mediamtx.yml:ro
      - mediamtx_recordings:/recordings
    networks:
      - gtvision_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "curl -sS -f http://localhost:9997/v3/paths/list || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  postgres_db:
    image: postgres:15-alpine
    container_name: gtvision_postgres
    env_file:
      - ./.env
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_INITDB_ARGS: "-E UTF8 --locale=C"
      POSTGRES_HOST_AUTH_METHOD: trust
      # Mude POSTGRES_MAX_CONNECTIONS conforme necessidade real
      POSTGRES_MAX_CONNECTIONS: 200
    ports:
      - "5432:5432"
    volumes:
      - gtvision_pg_data:/var/lib/postgresql/data
    networks:
      - gtvision_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis_cache:
    image: redis:7-alpine
    container_name: gtvision_redis
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    ports:
      - "6379:6379"
    volumes:
      - gtvision_redis_data:/data
    networks:
      - gtvision_network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  frontend:
    image: node:20-slim
    container_name: gtvision_frontend
    working_dir: /app
    shm_size: 1g
    volumes:
      - ./frontend:/app
      - /app/node_modules
    # Mantido o comando com --hostname
    command: sh -c "npm install --legacy-peer-deps && npm run dev -- --host 0.0.0.0 --port 5173"
    ports:
      - "5173:5173"
    networks:
      - gtvision_network
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    container_name: gtvision_nginx
    depends_on:
      - frontend
      - backend
      - gateway
      - mediamtx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - /dev/null:/etc/nginx/conf.d/default.conf
      - backend_static:/var/www/static:ro
      - backend_media:/var/www/media:ro
    networks:
      - gtvision_network
    restart: unless-stopped

volumes:
  gtvision_pg_data:
    driver: local
  gtvision_redis_data:
    driver: local
  backend_static:
    driver: local
  backend_media:
    driver: local
  mediamtx_recordings:
    driver: local

networks:
  gtvision_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16