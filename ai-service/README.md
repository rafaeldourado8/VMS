# GT-Vision AI Service

Microsservi√ßo isolado de IA para detec√ß√£o de ve√≠culos, placas (LPR) e classifica√ß√£o de modelos.

## üéØ Caracter√≠sticas

- **YOLO v8** para detec√ß√£o de ve√≠culos
- **TensorFlow** para LPR e classifica√ß√£o de modelos
- **Redis** para fila ass√≠ncrona (suporta 250+ c√¢meras)
- **FastAPI** com endpoints s√≠ncronos e ass√≠ncronos
- **Prometheus** para m√©tricas
- **GPU support** (NVIDIA CUDA)

## üöÄ Quick Start

### 1. Configura√ß√£o

```bash
cp .env.example .env
# Edite .env conforme necess√°rio
```

### 2. Iniciar com Docker

```bash
docker-compose up -d
```

### 3. Testar

```bash
pip install requests opencv-python numpy
python test_client.py
```

## üì° API Endpoints

### Health Check
```bash
GET /health
```

### Detec√ß√£o S√≠ncrona
```bash
POST /detect
{
  "camera_id": 1,
  "image_base64": "base64_encoded_image"
}
```

### Detec√ß√£o Ass√≠ncrona
```bash
POST /detect/async
{
  "camera_id": 1,
  "image_base64": "base64_encoded_image"
}

# Retorna task_id, depois consultar:
GET /result/{task_id}
```

### Upload de Arquivo
```bash
POST /detect/upload?camera_id=1
Content-Type: multipart/form-data
file: image.jpg
```

### M√©tricas Prometheus
```bash
GET /metrics
```

## üìä Performance

- **Throughput**: ~250 detec√ß√µes/segundo (4 workers, GPU)
- **Lat√™ncia**: <100ms por frame (GPU), <500ms (CPU)
- **Queue**: Suporta 1000 tarefas pendentes

## üîß Configura√ß√£o Avan√ßada

### Ajustar Workers
```env
WORKERS=8  # Aumentar para mais throughput
```

### GPU Memory
```env
GPU_MEMORY_FRACTION=0.8  # Ajustar conforme VRAM dispon√≠vel
```

### Confidence Threshold
```env
CONFIDENCE_THRESHOLD=0.5  # Aumentar para menos falsos positivos
```

## üì¶ Modelos

Coloque seus modelos treinados em `./models/`:

- `lpr_model.h5` - Modelo de reconhecimento de placas
- `vehicle_classifier.h5` - Classificador de modelos de ve√≠culos

Se n√£o existirem, o servi√ßo usa placeholders.

## üîó Integra√ß√£o com Backend Principal

```python
import requests
import base64

# Enviar frame para detec√ß√£o
with open("frame.jpg", "rb") as f:
    img_b64 = base64.b64encode(f.read()).decode()

response = requests.post("http://ai-service:8000/detect", json={
    "camera_id": 1,
    "image_base64": img_b64
})

detections = response.json()["detections"]
for det in detections:
    print(f"{det['object_type']}: {det['confidence']:.2f}")
    if det['plate_number']:
        print(f"  Placa: {det['plate_number']}")
    if det['vehicle_model']:
        print(f"  Modelo: {det['vehicle_model']}")
```

## üìà Monitoramento

M√©tricas dispon√≠veis em `http://localhost:9090/metrics`:

- `detections_processed_total` - Total de detec√ß√µes processadas
- `detection_processing_seconds` - Histograma de tempo de processamento
- `detection_queue_size` - Tamanho atual da fila

## üêõ Troubleshooting

### GPU n√£o detectada
```bash
# Verificar NVIDIA drivers
nvidia-smi

# Verificar TensorFlow GPU
docker exec -it gtvision-ai python -c "import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"
```

### Alta lat√™ncia
- Aumentar `WORKERS`
- Reduzir `CONFIDENCE_THRESHOLD`
- Usar modelo YOLO menor (yolov8n.pt)

### Queue cheia
- Aumentar `MAX_QUEUE_SIZE`
- Adicionar mais workers
- Otimizar processamento downstream
